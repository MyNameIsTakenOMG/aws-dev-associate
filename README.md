# aws-dev-associate

## road map
- [table of lecture contents](#table-of-lecture-contents)
- [lecture hands on](#lecture-hands-on)
- [practice tests](#practice-tests)
- [review lectures and tests](#review-lectures-and-tests)

### table of lecture contents
- [aws iam](#aws-iam)
- [ec2](#ec2)
- [ec2 instance storage](#ec2-instance-storage)
- [HA and scalability](#ha-and-scalability)
- [rds and aurora and elasticache](#rds-and-aurora-and-elasticache)
- [route53](#route53)
- [vpc](#vpc)
- [s3](#s3)
- [aws cli sdk and iam roles and policies](#aws-cli-sdk-and-iam-roles-and-policies)
- [s3 advanced](#s3-advanced)
- [s3 security](#s3-security)
- [cloudfront](#cloudfront)
- [containers](#containers)
- [elastic beanstalk](#elastic-beanstalk)
- [cloudformation](#cloudformation)
- [aws integration and messaging](#aws-integration-and-messaging)
- [aws monitoring and troubleshooting and audit](#aws-monitoring-and-troubleshooting-and-audit)
- [lambda](#lambda)
- [dynamodb](#dynamodb)
- [api gateway](#api-gateway)
- [cicd](#cicd)
- [serverless application model](#serverless-application-model)
- [cdk](#cdk)
- [cognito](#cognito)
- [other serverless](#other-serverless)
- [advanced identity](#advanced-identity)
- [security and encryption](#security-and-encryption)
- [other services](#other-services)


#### aws iam

- iam users & groups
  - user can belong to multiple groups or no group
  - groups can not be nested
- **Global service**
- iam permissions:
  - users & groups can be assigned `policies`
  - in aws, apply the least privilege principle
  - users who belong to multiple groups can inherit permissions from multiple groups policies
- iam password policy
  - allow all iam users to change their own passwords
  - prevent password re-use
  - more...
- mfa: password + code
  - mfa software(authy, google authenticator, support mutliple tokens)
  - hardware(universal 2nd factor:U2F security key, support multi-users with a single key)
  - hardware key fob mfa device: gemalto
  - hardware key fob mfa device for aws govcloud(US): surepassID
- access aws
  - aws console: password + mfa
  - cli: access key
  - sdk: access key
- iam roles for services
- iam security tools
  - iam credentials report: account level
  - iam access advisor: user level



#### ec2

- ec2 capabilities:
  - renting ec2 virtual machines
  - storing data: EBS
  - distributing load: ELB
  - scaling services: ASG
- ec2 sizing & configuration
  - windows, macos, linux
  - cpu, ram
  - storage: instance-store, ebs & efs
  - network card: public ip address
  - security group: firewall
  - user data
- ec2 user data: bootstrap script
  - only run once at the instance first start
  - runs with the root user permissions
- instance types
  - general purpose
  - compute optimized: HPC, ML, gaming server, batching workloads
  - memory optimized: databases, in-memory databases, big unstructured data real-time processing
  - storage optimized: OLTP, databases, in-memory databases, data warehouse, distributed file systems
- security groups
  - only contain `allow` rules
  - stateful, incoming traffic --> outgoing traffic
  - can be attached to multi-instances
  - application timeout error--> security group
  - all inbound traffic is blocked by default
- classic ports:
  - 22: ssh, sftp
  - 21: ftp
  - 80: http
  - 443: https
  - 3389: rdp (remote desktop procotol -- login windows instance)
- ssh & ec2 instance connect
- ec2 purchaing options
  - on-demand
  - reserved instances
  - saving plans
  - spot instances
  - dedicated hosts
  - dedicated instance
  - capacity reservations (combined with RI, Saving Plans)


#### ec2 instance storage

- ebs volume
  - network drive
  - az-scoped
  - have a provisioned capacity
  - root volume: delete on termination(by default)
  - snapshots: no need to detach, but recommended
    - snapshot features:
      - archive
      - recycle bin
      - fast snapshot restore
- ami
  - customization of ec2 instances
  - region-scoped
- instance store:
  - better I/O
  - good for buffer, cache
- ebs volume types
  - gp2/gp3
  - io1/io2: support ebs multi-attach (up to 16 ec2)
  - st1: big data, data warehouse,...
  - sc1: infrequently access
  - gp and io can be used as root volumes
- efs:
  - multi-az
  - nfs protocol
  - for linux
  - encryption using kms
- efs performance & storage classes
  - performance mode
  - throughput mode
  - storage classes: storage tiers, availability


#### HA and scalability

- scalability: vertical, horizontal
- availability: goes hand in hand with horizontal scaling, (running system in at least 2 data centers)
- ELB
  - health check: done on a port and a endpoint
  - types:
    - CLB
    - ALB:
      - layer 7;
      - http/2 and websocket;
      - route to different target groups: ec2(or ec2 in asg), ecs, lambda, private ip
      - fixed hostname
      - to see the client ip, use header: `X-Forwarded-For`
    - NLB: layer 4 (udp, tcp); handle millions of requests per requests; has `one static ip per az`; support EIP(helpful for whitelisting ip addresses)
      - target groups: ec2, private ip, ALB,
      - health check: tcp, http, https
    - GWLB: layer 3; use `GENEVE` procotol on port `6081`
      - manage a fleet of 3rd party network virtual appliance in aws
      - firewalls, intrusion detection, deep packet inspection system
      - target groups: ec2, private ip
  - security group
  - sticky sessions: works for clb, alb, nlb
    - application-based cookies: created by targets(custom cookies) or load balancers(application cookies)
    - duration-based cookies: created by load balancers
  - cross-zone load balancing
    - alb: enabled by default
    - nlb: disabled by default, no free
    - clb: disabled
  - ssl/tls
    - can manage certs using acm
    - or upload your own certs
    - https listener:
      - must specify a default cert
      - can optionally add a list of certs for multi domains
      - client can use SNI(server name indication) to specify which hostname to reach
      - alb and nlb: support multi https listeners; use sni
      - clb: support only one ssl, 
  - ssl -- SNI
    - only works for alb and nlb, cloudfront
  - connection draining(deregistration delay): time to complete `in-flight requests` while the instance is de-registering or unhealthy 
- ASG
  - a launch template
  - cloudwatch alarms & scaling
  - scaling policies:
    - dynamic scaling:
      - target tracking
      - simple/step
    - scheduled scaling
    - predictive scaling
  - metrics for scaling: `CPUUtilization`, `RequestCountPerTarget`, `Average Network In / Out`, or custom metrics
  - scaling cooldowns: when a scaling activity happends, you are in the cooldown period(no launch or terminate instances)
    - use Golden AMIs
  - instance refresh:
    - update launch template and re-create all ec2 instances
    - using native feature: `instance refresh`
    - setting minimum healthy percentage
    - specify `warm-up` time
 

#### rds and aurora and elasticache

- RDS: relational db service
  - support multi different db engines, including aws aurora
  - managed service
  - cannot ssh to the underlying instances
  - storage auto scaling:
    - have to set `Maximum Storage Threshold`
  - read replicas:
    - up to 15
    - within az, cross az or cross region
    - replication is `async`
    - apps must update db connection string
    - replicas can be promoted to their own db
    - use case: data analytics
    - network cost: same-region, free; cross-region charged
  - multi-az (Disaster Recovery):
    - replication: `sync`
    - one DNS name
    - not used for scaling
    - just a standby
    - from single-az to multi-az: click `modify`, then a snapshot is created, then a new db is created, then a synchronization is established
  - rds proxy:
    - fully managed db proxy
    - pool and share db connections
    - reduce rds & aurora failover time
    - must be access from vpc (private)
- Aurora:
  - support mysql and postgres
  - HA & read scaling:
    - 6 copies across 3 az:
      - 4/6 needed for write
      - 3/6 needed for read
      - up to 15 read replicas
      - auto failover
      - cross-region replication
  - aurora db cluster:
    - writer endpoint
    - reader endpoint
  - rds & aurora security:
    - at-rest encryption
    - in-flight encryption
    - iam authentication
    - security groups
    - no ssh except on `rds custom`
    - audit logs can be enabled and sent to cloudwatch logs
- elasticache:
  - managed redis:
    - multi-az and auto failover
    - read replicas with HA
    - backup and restore
  - managed memcached:
    - mutli-node
    - no HA
    - no persistant
    - no backup and restore
    - multi-threading
  - cache must have an invalidation strategy to make sure only the most current data is used in there
  - cache implementation considerations:
    - outdated data
    - data changing slowly or rapidly
    - data structured well or not
  - lazy loading/ cache-aside/ lazy population:
    - cons: cache read miss cause 3 round trips; data could be stale 
  - write through -- add or update cache when db is updated:
    - cons: data missed until it is added or updated in the main db (using lazy loading to solve it); a lot of data will never be read since each time db is updated, it will also update the cache
  - cache evictions and TTL
    - occur in three ways:
      - delete explicitly
      - memory is full and it is not recently used
      - a TTL is set
    - use cases: leaderboards; comments; activity stream
    - consider to scale up or out if eviction happens too frequently
  - final words of wisdom:
    - lazy loading works for many cases, especially on the read side
    - write-through is usually combined with lazy loading as targeted for the queries or workloads
    - set a ttl is usually not a bad idea, except when using write-through
    - only cache things that make sense

  
#### route53

- dns terminologies:
  - domain registar
  - dns records
  - zone file: contains dns records
  - name server: resovle dns queries
  - TLD
  - SLD
- route53:
  - 100 SLA
  - health check
  - records:
    - domain/subdomain name
    - record type
    - value
    - routing policy
    - ttl
  - records types
    - A
    - AAAA
    - Alias
    - CNAME
    - NS
  - hosted zones
    - public
    - private
    - $0.5 per month per hosted zone
  - TTL:
    - except for Alias records
    - high ttl or low ttl
  - CNAME vs Alias
    - CNAME: not for TLD
    - Alias: TLD or non-TLD, point to aws resources
  - Alias records:
    - map hostnames to aws resources
    - no ttl
    - TLD or non-TLD
    - targets:
      - **Note**: not for ec2 instances
      - elb
      - cloudfront
      - api gateway
      - EB
      - s3 websites
      - vpc interface endpoint
      - global accelerator
      - route53 records in the same hosted zone
  - health check:
    - http health check only for public resources
    - but can integrate with cloudwatch metrics to monitor private resources
    - automated dns failover
      - monitor an endpoint:
        - around 15 global health checkers
        - can be configured to only check on the first 5120 bytes of the responses
      - calculated health checks (similar to composite alarms)
      - monitor cloudwatch alarms
  - traffic flow:
    - visual editor to manage complex routing decision trees
    - can be saved as traffic flow policy and applied to different route 53 hosted zones
  - routing policies
    - simple: no health check; route traffic to a single resource
    - weighted:
      - dns records must have the same name and type
      - health check
      - all zero, then traffic evenly distributed
      - if zero, then no traffic sent
      - weights no need to be 100 when summed up
    - latency-based:
      - health check (failover feature)
      - helpful when latency is a priority
    - failover(active-passive)
    - geolocation:
      - should have a default record
      - health check
    - geoproximity
      - shift traffic to resources based on **bias**
      - must use **route 53 traffic flow**
    - ip-based routing
      - routing is based on ip addresses
      - provide a list of CIDRs for clients
    - multi-value:
      - health check
      - same record name and type
- route 53 and other dns registar
  - register a domain name and then use route 53 name servers



#### vpc

- overview -- basics:
  - vpc: private network
  - public subnet
  - private subnet
  - route tables
- internet gateway
- nat gateway (or nat instances:self-managed)
- NACL & security group
  - NACL: allow or deny; subnet level; stateless
  - security group: only allow; ec2 instance/ENI; stateful
- vpc flow logs:
  - vpc flow logs, subnet flow logs, ENI flow logs
  - logs can go to s3, KDF, cloudwatch logs
- vpc peering:
  - connect two vpc, privately using aws network
  - must not have overlapping CIDR
  - not transitive
- vpc endpoints:
  - gateway endpoints: s3, dynamodb
  - interface endpoint: for DX, s2s vpn
    - public virtual interface
    - private virtual interface
- s2s vpn & DX
  - s2s vpn: on-prem to aws; use public internet; encrypted
  - DX: take a long time to build; private connection; physical connection; private network



#### s3

- buckets:
  - regional service
  - needs a global unique name
- objects
  - the key is the full path: prefix + object name
  - max object size: 5tb
  - muse use `multi-part upload` if size > 5gb
- s3 security:
  - user-based: iam policies
  - resources-based:
    - bucket policies
    - object access control list -- fine-grained
    - bucket access control list -- less common
  - encryption
- s3 static website hosting:
  - make sure the bucket policy allows public access
  - make sure the bucket name is the same as the record name
- s3 versioning
  - enable at bucket level
- s3 -- replication
  - must enable versioning
  - cross-region replication
  - same-region replication
  - asynchronous copy
  - using `s3 batch replication` to replicate existing objects
  - can replicate `delete marker`
  - there is no chaining replicaiton
- s3 storage classes
  - standard
  - infrequent access:
    - standard-IA
    - one zone-IA
  - glacier storage classes
    - instant retrieval
    - flexible retrieval
    - deep archive
  - intelligent-tiering


#### aws cli sdk and iam roles and policies

- ec2 instance metadata (IMDS)
  - allow ec2 to `learn about themselves` without using an iam role
  - can retrieve the iam role from the metadata, but not the iam policy
- IMDS v2 and IMDS v1
- MFA with CLI
  - must create a temporary session using `sts getsessiontoken` api call
- aws sdk
- aws limits (quotas)
  - api rate limits
  - service quotas (service limits)
- exponential backoff
  - use exponential backoff if you encounter `ThrottlingException`
  - retry logic has been included in aws sdk (not for conditional check)
    - must only implement the retry for 5xx errors and throttling
    - do not implement on 4xx client errors
- aws cli credentials provider chain
  - command line options
  - environment variables
  - cli credentials file -- aws configure: `~/.aws/credentials`, `~/.aws/config`
  - container credentials for ecs tasks
  - instance profile credentials for ec2 instance profiles
- aws sdk default credentials provider chain(java)
  - java system properties
  - environment variables: access_key, access_secret
  - default credential profile file: for example `~/.aws/credentials`
  - ecs container credentials for ecs containers
  - instance profile credentials for ec2 instances
- aws credentials best practices
  - never store aws credentials in your code
  - if working with aws, use iam roles
  - if working outside of aws, use environment variables/ named profiles
- sign aws api requests
  - if use sdk or cli, the requests are signed for us



#### s3 advanced

- moving between storage classes
  - using `lifecycle rules`
- lifecycle rules:
  - transition actions
  - expiration actions
  - can be create for certain prefix or object tags
- s3 analytics -- storage class analysis
  - recommendations for standard and standard IA
  - good first step to put together lifecycle rules
- s3 event notifications
  - sns
  - sqs
  - lambda
  - eventbridge:
    - advanced filtering options with json rules
    - multi-destinations
    - eventbridge capabilities
- s3 performance
  - 3,500 put/copy/post/delete per prefix
  - 5,500 get/head per prefix
  - multi-part upload: must use > 5gb
  - s3 transfer acceleration
  - s3 byte-range fetches
    - parallelize get requests
    - can be configured to only fetch part of the data
- s3 select & glacier select
  - less network transfer
  - can filter rows and columns
  - can retrieve less data using sql
- s3 user-defined object metadata & s3 object tags
  - we cannot search the object metadata or object tags
  - instead, we must use external db as a search index, such as dynamodb


#### s3 security

- object encryption
  - sse-s3: use the key managed and owned by aws s3
    - header: "x-amz-server-side-encryption": "AES256"
  - sse-kms: use aws kms to manage the key
    - header: "x-amz-server-side-encryption": "aws:kms"
    - audit using cloudtrail
    - kms limit
  - sse-c: encryption key is managed by you
    - https is a must
    - the key must be provided every request
    - aws only handle encryption
  - client-side encryption: cse
    - use client libraries like: Amazon S3 Client-Side Encryption Library
    - clients handle encryption and decryption
- encryption in transit
  - ssl/tls
  - bucket policy condition: `aws:SecureTransport:`
- default encryption vs bucket policies
  - **note**: bucket policies are evaluated before default encryption
- s3 cors
  - we need to enable correct cors headers for clients to fetch
- s3 MFA delete
  - only root account can enable/disable MFA delete
  - no need: enable versioning; list deleted versions
  - need: permanently delete version; suspend versioning
- s3 access logs
  - set a target bucket(different) for storing access logs
- pre-signed urls
  - use cli, sdk, console to create pre-signed urls
- s3 access points
  - has its own domain
  - an access point policy
- s3 access points -- vpc origin
  - between the vpc gateway endpoint or interface endpoint and s3 buckets
  - vpc endpoint policy must allow access to the target bucket and access point
- s3 object lambda
  - architecture:
    - client
    - s3 object lambda access point
    - lambda
    - s3 access point
  - use cases:
    - redacting personal info before send them back to the caller
    - converting data formats or resizing images

  
#### cloudfront

- overview:
  - cdn
  - DDoS protection, integrated with aws shield, aws waf
- origins
  - s3 buckets:
    - OAC
  - custom origins:
    - s3 websites
    - alb
    - ec2
    - any http backend
- cloudfront vs s3 cross-region replication
  - cloudfront: good for static content availablt anywhere
  - s3: good for dynamic content available in a few regions
- caching
  - at edge locations
  - identify each object using `cache key`
  - increase the cache hit ratio and decrease requests to the origin
  - invalidation: `CreateInvalidation` api
- cache key:
  - unique identifiers for each object
  - hostname + resource portion of the url
  - can add other elements: http headers, cookies, query strings using `cache policies`
- cache policies:
  - http headers:
    - none
    - whitelist
  - cookies
  - query strings
    - none
    - whitelist
    - inlude all-except
    - all (not good, too many)
  - control TTL, can set by the origin using `Cache-Control` header
  - custom policies support
  - **note**: all http headers, cookies, query strings in `cache key` are included in origin requests
- origin request policy
  - Specify values that you want to include in origin requests without
including them in the Cache Key (no duplicated cached content)
  - can include: http headers, cookies, query strings
  - ability to add cloudfront http headers and custom headers to an origin requst that was not included in the viewer request
  - support custom policy
- cache invalidations
  - we can force an entire or partial cache refresh by performing cloudfront invalidations
  - can specify invalidation path: `*`, `/images/*`
- cache behaviors
  - configure different settings based on the content type or path pattern: `/api/*`, `/*`
  - the default cache behavior is always the last to be process and is always `/*`
- geo restriction
  - restrict who can access your distribution:
    - allowlist
    - blocklist
- signed url / signed cookies (normally after authentication and authorization)
  - for example, only premium users can access paid shared content
  - can attach a policy with:
    - url expiration
    - ip ranges
    - trusted signers (which aws accounts can create signed url)
  - signed url: individual files
  - signed cookies: multi files
- cloudfront signed url vs s3 pre-signed url
  - cloudfront:
    - filter by ip, path, data, expire
    - define access path
    - only root user can manage it
    - utilize caching features
  - s3:
    - uses iam key
    - issue a request as the person who pre-signed the url
- cloudfront signed url process
  - signers:
    - a trusted key group(recommended)
    - an aws account that has a cloudfront key pair(not recommended, should not use root account)
  - can create multi key groups
  - generate key pair
- price classes
  - all
  - 200: most regions, exluding the most expensive ones
  - 100: only the least expensive ones
- multiple origins
  - route to different origins based on the content type
  - based on path pattern: `/images/*`, `/api/*`
- origin groups
  - increate HA and do failover
  - one origin group: one primary and one secondary origin
- field level encryption
  - protect sensitive user info for each request
  - encryption at the edge locations
  - specify fields(max 10) to encrypt and public key used for encryption
  - decryption at the web server side
- real-time logs
  - using KDS
  - choose:
    - sampling rate
    - specify fields and cache behaviors(path patterns)



#### containers

- overview:
  - apps are packaged in containers that can run on any os
  - docker image repositories:
    - docker hub
    - aws ecr
- docker vs virtual machines
  - docker containers are sharing the resources
- docker containers management on aws
  - aws ecs
  - aws eks
  - aws fargate: serverless container platform, working with ecs and eks
  - ecr: storing container images
- aws ecs
  - ec2 launch type
    - each ec2 must have ecs agent to register in the ecs cluster
    - aws takes care of starting / stopping containers
    - ecs tasks: launch ec2 instances on ecs clusters
  - fargate launch type
    - no ec2 instances to manage
    - just take care of task definitions
    - aws runs ecs tasks for your based on cpu / ram you need
    - to scale, just increase the number of tasks
  - iam roles
    - ec2 instance profile: (ec2 launch type only)
      - used by ecs agent
      - make api calls to ecs service
      - send logs to cloudwatch logs
      - pull images from ecr
      - reference data in secrets manager or ssm parameter store
    - ecs task role
      - each task has a role
      - defined in task definition
  - load balancer integrations
    - alb: most use cases
    - nlb: only for high thoughput
    - clb: not recommended
  - data volumes (efs: serverless)
    - mount EFS onto ecs tasks
    - works for ec2 and fargate types
    - **note**: s3 cannot be used as file system
  - ecs auto scaling
    - uses aws application auto scaling
    - based on : cpu, ram, or alb requests count per target
    - target tracking
    - step scaling
    - scheduling schaling
    - ecs auto scaling (task level)
    - ec2 auto scaling (instance level)
  - ec2 launch type -- auto scaling ec2 instances
    - scaling by adding ec2 instances
    - asg
    - ecs cluster capacity provider: paired with asg
  - rolling updates
    - min: minimum healthy percent (relative to the actual running capacity: 100%) 
    - max: maximum percent (actual running capacity + allowed to create tasks: extra)
  - integrated with eventbridge:
    - on-schedule
    - on-demand
  - integrated with sqs
  - task definitions:
    - json form to tell ecs how to run a docker container
    - cpu, ram, iam role, image, port, logging, env, networking
    - up to 10 containers in one task definition
  - load balancing:
    - ec2 launch type:
      - dynamic host port mapping if the container port is defined in the task definition
      - alb will find the right port
      - must allow ec2 security group to any port from alb security group
    - fargate
      - each task has a private ip
      - only container port
      - one iam role per task definition
  - environment variables
    - hardcoded
    - ssm parameter store
    - secrets manager
    - env files(bulk) -- s3
  - data volumes (bind mounts)
    - share data between multi container in the same task definition
    - works for both ec2 and fargate
  - task placement (only for ec2 launch type)
    - task placement contraints
      - distinctInstance: tasks are placed on different instances
      - memberOf: placed on ec2 instances with a specified expression; or uses the cluster query language(advanced)
    - task placement strategies: best effort
      - binpack: aim the least available amount of cpu and ram
      - random
      - spread: tasks are spread evenly based on the specified value: az
      - or mix them up
    - task placement process:
      - identify the proper ec2 instance
      - identify the instance with task placement contraints
      - identify the instance with task placement strategies
      - select the instance
- ecr
  - container images registry
  - integrated with ecs
  - public and private
- aws copilot
  - help run apps on AppRunner, ECS, and fargate
  - help focus on building apps rather than setting up infra
  - provision all required infra for containerized apps
  - auto-deploy using codepipeline
  - deploy to multiple env
  - troubleshooting, logs, health status
- aws eks
  - aws managed kubernates cluster
  - eks supports ec2 and fargate
  - cloud-agnostic
  - collect logs and metrics using cloudwatch container insights
  - Node Types:
    - managed node group: create and manage nodes(ec2 instances) for you
    - self-managed nodes: you create nodes and register to the cluster and then managed by asg
    - aws fargatea
  - data volumes (file systems)
    - need to specify storageClass manifest on your eks cluster
    - leverages a container storage interface compliant driver
    - support:
      - ebs
      - efs (works with fargate)
      - fsx for lustre
      - fsx for netapp ontap



#### elastic beanstalk

- overview:
  - provision underlying resources: elb, asg, ec2, rds,...
  - managed service
  - web server tier & worker tier
  - devs only need to take care of application codes
  - still have full control over the configuration
- components
  - applications
  - application versions
  - environments: each application version
- deployment modes:
  - single instance
  - HA
- deployment options for updates
  - all at once
  - rolling (manual rollback)
  - rolling with addtional batches (manual rollback)
  - immutable: create a new asg with new versions, then swap to the new asg; high cost, long deployment, quick rollback(just terminate the asg, good for prod)
  - blue/green: (not a direct feature, need to work with route53 using weight policy and then swap urls when ready) create a new environment and switch over when ready
  - traffic splitting: (using the alb to do traffic splitting) canary testing -- send small portion of traffic to the new deployment. can trigger an automated rollback(very quick), new instances migrate to the original asg, old verion will be terminated. deployment health is moinitored
- deployment process
  - describe dependencies
  - package code as zip and describe dependencies
  - use console or cli to upload zip and then deploy
- lifecycle policy
  - EB can store up to 1000 app versions
  - to phase out old app versions, use lifecycle policy
    - based on time
    - based on space
  - the in-use version cannot be deleted
  - Option not to delete the source bundle in S3 to prevent data loss
- extensions
  - a zip file containing our code must be deployed to EB
  - all parameters set in UI can be configured with code using files
  - requirements:
    - must be in the path `.ebextensions/` directory in the root of source code
    - yml/json
    - .config extensions (exmaple: logging.config)
    - able to add other aws resources
    - able to modify some default settings using : `option_settings`
- under the hood
  - using cloudformation
  - can define cloudformation resources in your .ebextensions directory
- cloning
  - clone an environment
  - all resources and configuration are preserved
    - data in db is not preserved
- EB migration: load balancer
  - the load balancer type cannot be changed(clb, nlb,alb)
  - create a new environment with different type of load balancer, then swap CNAME or route 53 update
- EB with RDS
  - database lifecycle is tied to the EB environment lifecycle
  - good practice is to create a rds instance separately and provide our environment with the connection string
- EB migration: decouple RDS
  - create a snapshot of db (safeguard)
  - go to rds console to enable deletion protection
  - create a new environment but without rds, instead, just point to the existing rds instance
  - then perform CNAME swap (blue/green) or route53 update
  - terminate the old environment (rds will stay)
  - delete the cloudformation stack(in DELETE_FAILED state, cuz the rds unable to delete)


#### cloudformation

- benefits:
  - infra-as-code: good for version control
  - cost: good for cost management using tags (saving strategy)
  - productivity: quickly recreation
  - separation of concerns: multi stacks
- process:
  - template
  - s3 bucket
  - aws cloudformation
  - create stack to provision resources
- deployment
  - manual
  - automated
- cloudformation -- resources
- cloudformation -- parameters
  - to reference a parameter: using `!Ref`
  - pseudo parameters (provided by aws)
- cloudforamtion -- mappings
  - use `!FindInMap` to return a value from the map
- cloudformation -- outputs
  - declare optional outputs
  - using `Export` section
  - using `!ImportValue` to reference
- cloudformation -- conditions
  - intrinsic functions(logical)
  - can be applied to resources, outputs,... but not `parameters`
- cloudformation -- intrinsic functions
  - !GetAtt: get the attributes of the resources
  - !Base64: use to encode user data for ec2 for example (convert string to base64)
  - condition functions
- rollbacks
  - creation fails
  - update fails
  - rollback fails: manual fix, then call `ContinueUpdateRollback` api to continue rollback
- service role
  - iam users with the iam role which include `iam:PassRole` permission
  - make sure iam users have the least privileges
- cloudformatin capabilities
  - CAPABILITY_NAMED_IAM & CAPABILITY_IAM
  - CAPABILITY_AUTO_EXPAND
  - InsufficientCapabilitiesException
- deletion policy -- for stack resources
  - delete: won't work with s3 bucket if it is empty
  - retain: work with resources
  - snapshot
- stack policy -- for stack updates
  - define update actions that are allowed during stack updates
  - must explicitly allow
- termination protection -- for stack deletes
- custom resources
  - define custom resources
  - have custom logic (scripts) via lambda
  - service token specify where to send the requests(lambda, sns)
  - use case: empty s3 bucket before deleting it
- stacksets
  - create, update, delete stacks across multi accounts and regions
  - can be applied within an aws organization


#### aws integration and messaging

- decouple your applications(scale independently from our applications):
  - sqs: queue model
  - sns: pub/sub model
  - kinesis: real-time streaming model
- sqs:
  - standard queue:
    - unlimited throughput, unlimited number of messages in queue
    - low latency
    - 256kb max size of each message
    - retention: 4 - 14 days
    - best-effort, at least once delivery
  - producing messages
    - sendMessage api call
    - message is persisted until a consumer deletes it
  - consuming messages
    - ec2, lambda,...
    - poll messages (up to 10)
    - process messages
    - delete message using deleteMessage api call
  - multi ec2 consumers
    - poll and process messages in parallel
    - scale consumers horizontally to increase throughput
  - with asg
    - scaling based on an alarm set on the cloudwatch metric: ApproximateNumberOfMessages
  - decouple between application tiers
  - security
    - encryptioin in-transit and at rest
    - or client side encryption
  - queue access policy
    - resource-based policy
  - message visibility timeout
    - a message is invisible when it is polled
    - by default 30 seconds, then after that, it becomes visible again
    - consumer call the ChangeMessageVisibility api    
  - dead letter queue
    - a message will be sent to a dlq after being failed to be processed
    - configure `MaximumReceives`
    - standard queue -- standard dlq queue
    - fifo queue -- fifo dlq queue
    - good to set retention of 14 days in the dlq
    - redrive to source
      - when our code is fixed, we can redrive messages from dlq to the source queue(or any other queue) in batches without writing custom code
  - delay queue
    - delay a message up to 15 mins
  - long polling
    - can optionally wait if there is no message in the queue
    - decrease the number of api calls
    - is preferable to short polling
    - can enable at queue level
  - extended client
    - max size of each message is 256kb
    - to send data like 1GB, use `SQS extended client`(java library)
  - must know api
    - CreateQueue
    - DeleteQueue
    - PurgeQueue
    - SendMessage, ReceiveMessage, DeleteMessage
    - MaxNumberOfMessages (up to 10, for receiving messages api)
    - ReceiveMessageWaitTimeSeconds: long polling
    - ChangeMessageVisibility
    - batch apis for sending, deleting, changing messages visibility: help decrease your costs
  - fifo queue
    - limited throughput: 300 per second, 3000 per second with batch
    - exactly once delivery
    - messages processed in order
    - deduplication
      - interval is 5 min
      - two methods:
        - Content-based deduplication: will do a SHA-256 hash of the message body
        - Explicitly provide a Message Deduplication ID
      - prevent producers from sending duplicated messages
    - message grouping
      - same value for MessageGroupID in fifo queue
      - ordering inside the group is kept
      - ordering across groups is not guaranteed
      - each group has different consumer
- aws SNS
  - pub/sub: send one message to many receivers
  - one event producer can only send messages to one sns topic
  - many event receivers or subscribers can listen to one sns topic
  - Each subscriber to the topic will get all the messages (note: new feature to filter messages)
  - integrated with many aws services as message senders
  - how to publish
    - topic publish using sdk
    - direct publish for mobile apps sdk
  - security: similar to sqs
  - sns + sqs: fan out
    - can add more sqs subscribers over time
  - fifo topic
    - similar to fifo queue
    - message group id (all messages ordered in the same group)
    - deduplication
    - **note**: can have sqs standard or fifo as subscribers
    - limited throughput
  - message filtering
    - subscribers use it to filter messages based on their needs
    - otherwise all message will be received
- aws kinesis
  - collect, process, and analyze streaming data in real-time
  - kinesis data streams
    - retention: 1-365 days
    - support replay
    - data in kinesis is immutable, cannot be deleted
    - data with the same partition key goes to the same shard (ordering)
    - capacity modes
      - provisioned mode:
        - choose number of shards
        - each shard: 1mb in , 2mb out per second
      - on-demand mode:
        - default capacity provisioned: 4mb in or 4000 records per second
    - security
      - using iam policy
      - encryption in-transit and at rest or choose client-side encryption
      - vpc endpoint is available for kinesis
      - monitoring using cloudtrail
    - producers
      - data record: sequence number; partition key, data blob
      - aws sdk, kinesis producer libracy(KCL), kinesis agent
      - putRecord api
      - 1mb or 1000 records per sec per shard
      - use batching with putRecord api
      - use highly distributed partition key to avoid hot partition
      - `ProvisionedThroughputExceeded`
        - use highly distributed partition key
        - increase shards
        - retry with exponential backoff
    - consumers
      - max 5 consumers per shard
      - lambda, kda, kdf, kcl, custom consumer(aws sdk -- classic or enhanced fan-out)
      - custom consumer
        - classic: multi consumers share 2mb out; max 5 getRecord api/sec
        - enhanced fan-out: multi consumers have 2mb out each
      - lambda
        - support classic and enhanced fan-out consumers
        - read records in batches
        - can configure batch size and batch window
        - retry
        - can process up to 10 batches per shard simultaneously
      - KCL
        - java library
        - each shard to be to read by only one kcl instance
        - progress is checkpointed into dynamodb
        - can run on ec2, on-prem, EB
        - track other workers and share the work amongst shards using dynamodb
        - records are read in order at shard level
        - one kcl can read multi shards
    - kinesis operation -- shard splitting
      - used to increase stream capacity (1mb in per shard)
      - divide hot shard
      - no auto scaling, just manual
      - cannot split into more than 2 shards at a time
      - the old shard will be closed and deleted once the data is expired
    - kinesis operation -- merging shards
      - decrease the stream capacity and save costs
      - group two shards with low traffic(cold shards)
      - cannot merge more than 2 shards at a time
      - the old shard will be closed and deleted once the data is expired
  - kinesis data firehose
    - fully-managed, auto scaling, serverless
    - pay for data going into firehose
    - near real time
      - buffer size: minimum 1mb
      - buffer interval: 0 - 900 sec
    - support data multi format, conversions, transforms, and compression
    - support custom data transform using lamdba
    - can send failed data to s3 bucket
    - no data storage
    - no replay
  - kinesis data analytics
    - sql application:
      - real time analytics on kds and kdf using sql
      - add reference data from s3 to enrich streaming data
      - fully-managed, serverless
      - auto scaling
      - pay-as-you-go
      - output: kds, kfd
    - apache flink( java, scala, or sql)
      - source: kds, aws msk
      - no kdf
  - kinesis video streams
  - ordering data into kinesis using partition key
  - ordering data into sqs using fifo queue (no need group ID, unlike sns), with `only one consumer`
    - to add more consumers, use group ID, similar to partition key in kinesis
  - kinesis vs sqs ordering



#### aws monitoring and troubleshooting and audit

- importance of monitoring
  - deploy applications:
    - safely
    - automated
    - infra-as-code
    - leverage aws managed services
  - app users:
    - latency
    - outage
    - communicate with it support
    - troubleshooting and remediation
  - internal monitoring
    - can we prevent issues before they happen
    - performance and cost
    - trends(scaling patterns)
    - learning and improvement
- monitoring in aws
  - aws cloudwatch
    - metrics
    - logs
    - events
    - alarms
  - aws x-ray
    - troubleshooting application performance and errors
    - distributed tracing of microservices
  - aws cloudtrail
    - api calls
    - auditing
- cloudwatch metrics
  - for every aws service
  - metric is a variable
  - belong to namespaces
  - determined by dimensions
  - have timestamps (at second granularity, cloudwatch will aggregate multi data points if they are in the same timestamp)
  - can create custom dashboards
- ec2 detailed monitoring
  - 1 min interval
  - will scale faster when using asg
  - aws free tier allows 10 detailed monitoring metrics
- custom metrics
  - possible to send custom metrics
  - example, memory usage, disk space, number of logged in users,...
  - can use dimensions to segment metrics
  - metric resolution:
    - standard: 1min
    - high resolution: 1/5/10/30 seconds
    - **note**: accept metric data points two weeks in the past and 2 hrs in the future
- cloudwatch logs
  - log groups
  - log stream
  - log expiration
  - cloudwatch logs can be sent to s3, kds, kdf, lambda, opensearch
  - encrypted by default
  - can set kms custom key
  - sources:
    - sdk, cloudwatch agent,
    - EB
    - ecs
    - aws lamdba
    - api gateway
    - cloudtrail
    - route53
- cloudwatch log insights
  - search and analyze log data
  - can query multi log groups in different aws accounts
  - it is a query engine, not a real-time engine
- cloudwatch log -- s3 export
  - can take up to 12 hrs, not near real time
- cloudwatch log subscriptions
  - real-time log events
  - lambda, kdf, kds
  - subscription filter
  - cross-account subscription
- cloudwatch logs aggregation multi-account & multi-region
- cloudwatch logs for ec2
  - need to run cloudwatch agent with iam permission
  - cloudwatch agent can be set on-prem
- cloudwatch logs agent & unified agent
  - unified agent: new version
    - centralized configure using ssm parameter store
    - collect additional system-level metrics
    - by default: disk, cpu, network
- cloudwatch logs metric filter
  - can use filter expression
  - not retroactive
  - able to specify up to 3 dimensions for metric filter
- cloudwatch alarms
  - triggered by any metric
  - period:
    - length of time to evaluate the metric
    - high resolution custom metrics: 10, 30, or multi of 60 sec
  - targets:
    - ec2
    - ec2 auto scaling
    - sns
  - composite alarms
    - monitor multi alarms states
    - help reduce alarm noise
  - ec2 instance recovery:
    - status check
    - recovery: same private, public, EIP, metadata, placement group
  - good to know
    - alarms can be created on cloudwatch logs metric filter
  - synthetics canary
    - aws version of e2e tests
    - integrated with alarms
    - blueprints:
      - heartbeat monitor
      - api canary
      - broken link checker
      - visual monitoring
      - canary recorder
      - GUI workflow builder
- aws eventbridge
  - schedule: cron jobs
  - event patterns
  - trigger lamdba, send sqs, sns,...
  - event buses
    - can be accessed by other aws accounts
    - can archive event
    - can replay event
  - schema registry
    - eventbridge can analyze and infer the schema
    - allow to create custom registry
    - can be versioned
  - resource-based policy
  - multi-account aggregated
- aws x-ray
  - great for distributed system with a plenty of microservices
  - review request behavior
  - check service sla
  - check if throttled
  - integration:
    - lambda
    - EB
    - ecs
    - elb
    - api gateway
    - ec2 or on-prem
  - leverages tracing
    - tracing: e2e way to follow a request
    - each service will add its own `trace`
    - tracing is made of segments (subsegments)
    - annotations: add extra info
    - security: iam, kms
    - able to trace:
      - every request
      - sample request (%, for example 3%)
  - to enable x-ray
    - using aws x-ray sdk
    - each application must have iam rights to write data to x-ray
  - x-ray magic:
    - collect data from different services
    - service map is computed from all segments and traces
    - x-ray is graphical, non-technical people can help
  - troubleshooting:
    - on ec2:
      - check the iam permission
      - check the x-ray daemon
    - on lambda:
      - check iam permission
      - enable lambda x-ray active tracing feature
      - ensure x-ray client initialized in the code
  - intrumentation in the code
    - the measure of product’s performance, diagnose errors, and to write trace information.
    - to instrument your application code, using sdk
    - can custom annotations using interceptors, filters, handlers, middlewares,...
  - concepts:
    - segments
    - subsegments
    - trace
    - sampling: decrease the amount of request sent to x-ray
    - annotations: key-value pairs used to `index` traces with `filters`
    - metadata: not indexed
  - sampling rules:
    - control the amount of data sent to x-ray
    - By default, the X-Ray SDK records the first request `each second`, and `five percent` of any additional requests.
    - `One request per second is the reservoir`, which ensures that at least one trace is recorded each second as long the service is serving requests.
    - `Five percent is the rate` at which additional requests beyond the reservoir size are sampled.
  - custom sampling rules
  - x-ray write apis
  - x-ray read apis
  - x-ray with elastic beanstalk
    - EB platform include x-ray daemon
    - can enable by setting an option via eb console or with a configuration file (in the .ebextensions/xray-daemon.config)
    - remember to check the ec2 iam permission and x-ray sdk
    - **note**: x-ray daemon is not provided for multi-container docker
  - ecs + x-ray
    - x-ray daemon as a separate container
    - x-ray daemon as a sidecar container with the main container
    - for fargate cluster
      - x-ray daemon as a sidecar container
- aws distro for openTelemetry
  - open source version of aws x-ray
  - a standard set of apis to multiple destinations 
- aws cloudtrail
  - enabled by default
  - get a history of events/api calls made within aws account
  - a trail can be applied to all regions or a single region
  - Provides governance, compliance and audit for your AWS Account
  - cloudtrail events
    - management events (enabled by default), can separate `read events` and `write events`
    - data events (not enabled by default due to a large amount of operations)
    - cloudtrail insights events (findings)
  - event retention: 90 days, or choose to send them to s3 bucket
  - integration with eventbridge



#### lambda

- benefits:
  - cheap
  - integrated with a lot aws servcies
  - support multi programming languages
  - increase ram will improve cpu and network
    - once reach 1,792 mb for ram, we have a whole vCPU,
    - after 1,792mb, consider using multi-threading 
- synchronous invocation
  - cli, sdk, alb, api gateway
  - user invoke
  - service invoke
- integrated with alb
  - lambda function must be registered in a target group
- alb + lambda -- permission
  - lambda resource policy
- alb multi-header values
  - When you enable multi-value headers, HTTP headers and query string parameters that are sent with multiple values are shown as arrays within the AWS Lambda event and response objects.
- asynchronous invocations
  - s3, sns, eventbridge, ses, IoT,...
  - events are placed in an event queue
  - make sure the processing is idempotent
- s3 event notifications
  - simple s3 event pattern -- metadata sync
- event source mapping
  - kds
  - sqs, sqs fifo
  - dynamodb stream
  - Common denominator: records need to be polled from the source
  - lambda function is invoked synchronously
- streams and lambda (kinesis and dynamodb)
  - An event source mapping creates an iterator for each shard, processes items in order
  - configure batch size and batch window to lower traffic
  - can also process up to 10 batches in parallel
  - in-order processing is still guaranteed for each partition key
  - error handling
    - by default, one single error will cause whole batch to be reprocessed
    - but can configure to use `partial batch response`
    - discard events can go to a destination
- event source mapping sqs & sqs fifo
  - recommend: set queue visibility timeout to 6x the timeout of your lambda function
  - to use a dlq
  - support fifo queues, scaling up to the number of active message groups
  - Lambda deletes items from the queue after they're processed successfully.
  - Occasionally, the event source mapping might receive the same item from the queue twice, even if no function error occurred.
  - You can configure the source queue to send items to a dead-letter queue if they can't be processed.
- lambda event mapper scaling
  - kinesis data streams and dynamodb stream
    - one lambda invocation per stream shard
    - if in parallel, up to 10 batches processed per shard simultaneously
  - sqs standard
    - add 60 lambda instances per minute to scale
    - up to 1000 batches in parallel
  - sqs fifo
    - group ID
    - scale to the number of active message groups
- lambda event and context object
  - event:
    - contains the data(from calling service) to be processed
  - context:
    - provides info about the invocation, function, and runtime environment
- lambda -- destinations
  - can configure to send result to destination
  - `asynchronous invocations`:
    - sqs
    - sns
    - lambda
    - eventbridge bus
  - event source mapping: for discarded event batches
  - **note**: events can be sent to dlq directly from sqs
- lambda execution role
  - grant lambda to access other aws resources
  - when using event source mapping, lambda uses a role to read event data
- resource-based policies
  - grant lambda function access to other services or aws accounts
- environment variables
  - adjust the function behavior without updating code
- lambda logging & monitoring
  - cloudwatch logs: make sure lambda functions has permission to write logs to cloudwatch logs
  - cloudwatch metrics
- tracing with x-ray
  - enable x-ray active tracing
  - lambda does have a x-ray daemon running under the hood
  - make sure use x-ray sdk and initialize it
  - make sure the lambda has the right permission for x-ray (a managed policy)
- customization at the Edge
  - edge functions:
    - a code attached to the cloudfront distro
  - cloudfront provides two types:
    - cloudfront functions
    - lambda@edge
    - already deployed globally
    - fully managed
    - customize cdn content
    - use cases:
      - website security and privacy
      - seo
      - bot mitigation
      - a/b testing
      - auth
      - ...
  - cloudfront functions
    - lightweight functions
    - for high-scale, latency-sensitive cdn customization
    - viewer request: cloudfront receives the requests from clients
    - viewer response: before cloudfront forwards the response to the viewer
    - a native feature
  - lambda@edge
    - scale to 1000 requests per second
    - used to change cloudfront requests and responses:
      - viewer requests: after cloudfront receives the request from client
      - viewer response: before cloudfront forwards the response to the client
      - origin requests: the request before cloudfront forwards to the origin
      - origin response: after cloudfront receives the response from the origin
    - author your lambda in one aws region, then cloudfront replicate to its locations 
  - cloudfront function vs lambda@edge
    - cloudfront function: cache key normalize; header modify; url rewrite; auth
    - lambda@edge: longer execution time, code depends on a 3rd libraries; able to access external services; file system access
- lambda with VPC
  - by default, a lambda is outside of your vpc, in the aws-owned vpc. thus cannot access private resources
  - then must specify subnet and sg
  - then lambda will create an EIP
  - internet access:
    - deploy in a public subnet does not give it a public ip
    - deploy in a private subnet + nat gateway and internet gateway
- lambda function configuration
  - ram:
    - at 1792mb, one full vCPU
    - after 1792mb, need to use multi-threading in your code
  - if computation heavy, then increase ram
- lambda execution context
  - temporary runtime environment
  - great for database connection, http clients or sdk client initialization
  - will be maintained for some time for other lambda invocation (re-use)
  - the context includes the /tmp directory
- the /tmp directory
  - download a big file
  - need disk to do some task
  - max size 10gb
  - remain when execution context is frozen
- lambda layers
  - externalize dependencies for re-use
- lambda file system mounting
  - if lambda is in a vpc, then it can access efs
  - must use efs access point
  - limit: efs connection limit and connection burst limits
- lambda concurrency and throttling
  - limit: 1000 concur
  - can set `reserved concur` at function level
  - can increase limit by opening a ticket
- cold starts & provisioned concur
  - provisioned: is already allocated before the function gets called. no cold start
- function dependencies
  - the aws sdk comes by default for every lambda invocation, so no need to bundle the aws-sdk
- lambda and cloudformation
  - inline: use Code.ZipFile, cannot have dependencies
  - through s3: must specify the s3 zip location
- lambda container images
  - deploy lambda function as a container image (up to 10gb) to ecr
  - can test the container locally using lambda runtime interface emulator
  - unified workflow to build apps
  - best practices:
    - use aws-provided base image
    - use multi-stage builds
    - build from stable to frequently changing
    - use a single repo for functions with large layers
- lambda versions
  - the $latest
  - versions: immutable, have their own arn
- lambda aliases
  - pointers to lambda versions
  - enable canary deployment
- lambda & codedeploy
  - codedeploy help automate traffic shift for lambda alias
  - integrated with SAM
  - deployment strategies:
    - linear: increase traffic every x min to 100%
    - canary: try x then 100%
    - allAtOnce
  - appspec yml
    - name
    - alias
    - currentversion
    - targetversion
- lambda -- function url
  - can be applied to alias or $latest
  - throttle your function by using reserved concur
  - security:
    - resource-based policy
    - cors
    - authType: configure public access
      - none: allow public access and unauthenticated access
        - resource-based policy is always in effect
      - aws_iam: identity-based policy and resource-based policy are evaluated
        - same account: Identity-based Policy OR Resource-based Policy as ALLOW
        - cross account: Identity-based Policy AND Resource Based Policy as ALLOW
- lambda and codeguru profiling
  - using codeguru profiler for your lambda function to check performance
  - support java and python
- aws lambda limits to know - per region
  - execution
    - max timeout: 15 min
    - environment variables: 4kb
  - deployment
    - zip: 50mb
    - uncompressed: 250mb
    - can use /tmp to load other files at startup
- lambda best practiecs
  - perform heavy-duty work outside of your lambda
  - use env variables
  - minimize deployment package size
  - avoid using recursive code, never have a lambda call itself



#### dynamodb

- nosql db:
  - not support JOIN
  - distributed
  - all the data needed for a query is present in one row
  - no aggregation operations
  - scale horizontally
- overview
  - fully managed
  - HA and scalable
- basics
  - max size of each item: 400kb
  - support: scalar types, document types, set types
- primary key
  - partition key (hash)
  - partition key + sort key (hash + range)
  - must be unique for identifying an item
- read/write capacity modes
  - provisioned mode:
    - read capacity units -- throughput for reads
    - write capacity units -- throughput for writes
    - option for auto-scaling
    - if gets throttled, try `exponential backoff`
  - on-demand mode:
    - read/write auto scaling
    - good for unpredictive workload
  - One Write Capacity Unit (WCU): represents one write per second for an
item up to 1 KB in size. if more than 1kb, then more wcus
  - One Read Capacity Unit (RCU): represents one Strongly Consistent Read per second, or two Eventually Consistent Reads per second, for an item up to 4 KB in size
  - can switch between every 24 hrs
- strongly consistent read vs eventually consistent read
  - eventually consistent read (default)
  - strongly consistent read: set `consistentread` parameter; will consume double rcu
- partitions internal
  - data stored in partitions
  - Partition Keys go through a hashing algorithm to know to which partition they go to
  - to compute the number of partitions:
    - by capacity
    - by size
    - then check the max one (by capacity, by size)
    - wcu and rcu are spread evenly across partitions
- throttling
  - reasons:
    - hot keys
    - hot partitions
    - very large items (rcu, wcu depend on the size of item)
  - solutions:
    - exponential backoff
    - distribute partition keys more
    - if RCU issue, can use dynamodb accelerator DAX
- writing data
  - putItem: same primary key will be overwritten
  - updateItem
  - conditional write: help with concurrent access, no performance agent
- reading data
  - getItem:
    - eventually consistent read (default)
    - hash / hash + range
    - projectionExpression
  - query:
    - keyConditionExpression
    - filterExpression: after query operation, only used with non-key attributes
    - return up to 1mb data
    - able to paginate
  - scan:
    - consumer a lot rcu
    - can perform `parallel scan`
    - can use `projectionExpression` and `filterExpression`, but no change to rcu
- deleting data
  - delete item
  - delete table
- batch operations
  - allow to reduce the number of api calls
  - operations are done in parallel
  - for partial failure, we need to retry them
  - batchwriteitem:
    - up to 25 items
    - unprocessedkeys
  - batchgetitem:
    - up to 100 items
    - unprocessedkeys
- dynamodb -- partiql
  - sql compatible query language for dynamodb
  - using sdk, cli, aws console, dynamodb api,
- conditional writes
  - attribute_exists
  - attribute_not_exists: make sure item is not overwritten, used for partition key or partition key and sort key
  - attribute_type
  - contains (string): check substring
  - begins_with (string): check prefix
  - size (string length)
  - in, and, between,
- local secondary index
  - alternative sort key (same partition key)
  - up to 5 local secondary indexes
  - must be defined at the table creation time
  - can choose which attribute to project -- keys_only, all, include 
- global secondary index
  - alternative primary key (partition key or partition key + sort key)
  - query on non-key attributes
  - can choose which attribute to project -- keys_only, all, include
  - can be added/updated after table creation
  - should provision rcu, wcu 
- indexes and throttling
  - GSI:
    - if write is throttled on gsi, then main table will be throttled
    - even if wcu on main table is fine!!!!
    - choose gsi partition key carefully
    - assign wcu carefully
  - LSI:
    - use wcu and rcu of the main table
    - no special throttling considerations
- partiql
  - sql-compatible
  - support batch operations
- optimistic locking
  - ddb has a feature called 'conditional writes'
  - check an attribute, like `version number`
  - optimistic concurrency control (OCC)
- dynamodb  accelerator (DAX)
  - fully managed service
  - seamless in-memory cache for ddb
  - no application logic modification
  - help offload read workload, solve the hot key issue
  - up to 10 nodes in the cluster
  - multi-az
  - secure
  - vs elasticache
    - store individual objects cache
    - elasticache: store aggregation result
- dynamodb streams
  - ordered stream of item-level modifications in a table
  - downstream:
    - kds
    - lambda
    - kcl
  - retention: 24 hrs
  - use cases:
    - cross-region replication
    - into opensearch
    - analytics
    - react to changes in real-time
  - able to choose what information will be written into stream
    - keys
    - new
    - old
    - new and old
  - made of shards, just like kinesis data streams
  - with lambda
    - using event source mapping
    - invoked  synchronously
- time to live
  - auto delete item after an expiry timestamp
  - no extra cost
  - must be a "number" data type of "unix epoch timestamp" value
  - deletion within 48 hrs
  - the deletion operation for each expired item enters the dynamodb stream
- dynamodb cli
- dynamodb transactions
  - ACID
  - read modes
  - write modes
  - consume 2x wcu and rcu
    - dynamodb perform 2 operations for every item (prepare and commit)
  - two operations
    - transactgetitems
    - transactwriteitems
- ddb as session state cache
  - common use
  - elasticache: not serverless
  - efs: must attached to ec2 as network drive
  - ebs and instance store: only for local cache
  - s3: not for small objects, high latency
- write sharding
  - add suffixes to make partition keys more distributed
- write types
  - concurrent writes
  - atomic writes
  - conditional writes
  - batch writes
- large objects pattern
  - with s3, only store metadata
- indexing s3 object metadata
  - could use s3 event notification to call a lambda to store object metadata
- dynamodb operations
  - table cleanup
    - scan + delete: expensive
    - drop + recreate: fast and cheap
  - copying a dynamodb table
    - **aws data pipeline**
    - backup and restore: takes time
    - scan + putitem or batchwriteitem : write custom code
- security and other features
  - security:
  - backup and restore:
    - PITR
    - no performance impact
  - global table
  - dynamodb local: good for dev and test
  - aws database migration service: only for target, not for source
- fine-grained access control
  - for federated or cognito identity pool: get sts token
  - can assign iam role
  - leadingKeys -- limit row-level access on primary key
  - attributes -- limit specific attributes that users can see



#### api gateway

- integration:
  - lambda
  - http
  - aws service
- endpoint types
  - edge-optimized (default)
    - cloudfront edge locations
    - api gateway lives in one region
  - regional:
    - can manually combine with cloufront distro
  - private:
    - only within your vpc
- security:
  - iam roles
  - cognito
  - custom authorizer
- deployment stages
  - making changes for the api gateway does not mean they are effective
  - need to make a deployment for them to be in effect
  - each stage has its own configuration parameters
  - stage can roll back
- stage variables
  - like environment variables for api gateway
  - used in
    - lambda arn
    - http endpoint
    - parameter mapping templates
  - use cases:
    - configure http endpoints your stages talk to (dev,test,prod,...)
    - pass configuration parameters to lambda through mapping templates
  - stage variables are passed to the `context` object in lambda
  - format: ${stageVariables.variableName}
- api gateway stage variables & lambda aliases
  - create stage variables to indicate the corresponding lambda aliases
  - then api gateway will automatically invoke the right lambda function
- api gateway -- canary deployment
  - usually prod stage
  - split a certain percentage of traffic to the canary channel
  - this is the blue/green deployment with lambda and api gateway
- integration types
  - MOCK: returns a response without sending requests to the backend
  - HTTP/AWS(lambda or other services): setup data mapping using mapping template
  - AWS_Proxy (lambda proxy): incoming requests will be forwarded to the lambda
    - no mapping template, headers, query string parameters,... are passed as arguments
  - http_proxy:
    - no mapping template
    - http request passed to the backend
    - possible to add http headers
- mapping templates (aws & http integration)
  - used to modify request/response
  - add headers
  - modify body content
  - rename/modify query string parameters
  - uses Velecity template language(vtl) or javascript
  - filter out results (remove unnecessary data)
  - content-type: application/json or application/xml
  - can convert rest json to  SOAP xml, build the soap message and call soap service with the message, then transform soap response back to rest json response
- open api spec
  - can export current api as openapi spec
  - can be written in yml or json
  - using open api to generate sdk for our applications
- rest api -- request validation
  - can configure api gateway to perform basic validation without calling the backend
- caching api responses
  - defined per stage
  - possible to override cache settings per method
  - encryption option
  - expensive, use it only in prod
- cache invalidation
  - able to flush entire cache
  - invalidationCache policy to restrict access
- usage plans & api keys
  - make api profitable
  - usage plan:
    - who can access what api stages and methods
    - how mush they can use
    - uses api keys to identify api clients and meter access
    - configure throttling limits and quota limits for individual client
  - api keys:
    - distributed to your customers
    - can use with usage plan to control access
    - throttling limits are applied to api keys
    - quotas limits : overall number of requests
- correct order for api keys
  - to configure a usage plan:
    - create apis, configure methods that need api keys and deploy
    - generate api keys and distribute them to app developers(your customers)
    - create usage plan with the desired throttle and quota limits
    - associate api stage with api keys with usage plan
    - the header: `x-api-key`
- api gateway -- logging & tracing
  - cloudwatch logs:
    - configure log level
    - can override settings on a per api basis
  - x-ray
- api gateway -- cloudwatch metrics
  - cacheHitCount & cacheMissCount
  - count
  - integrationLatency
  - latency
  - 4xx and 5xx
- api gateway throttling
  - account limit
  - can set stage limit & method limits
  - or can define usage plans to throttle per customer
- api gateway -- errors
  - 4xx
  - 5xx
- api gateway -- cors
  - the options pre-flight request must contain the following headers:
    - access-control-allow-methods
    - access-control-allow-headers
    - access-control-allow-origins
- api gateway -- security
  - iam permission to iam roles iam users
  - resource policies
  - cognito user pools
  - lambda authorizer:
    - token-based authorizer
    - a request parameter-based lambda authorizer
- api gateway -- http api vs rest api
  - http apis
  - rest apis
- api gateway -- websocket api
  - bi-directional communication
  - client-server: connecitonID is re-used for different messages
  - server-client: connection url callback
    - post
    - get
    - delete
  - routing
    - no routes -- sent to $default
    - you select a route selection expression to select the field on the json to route from
- api gateway -- architecture
  - create a single interface for all the microservices in your company
  - apply a simple domain name and ssl
  - can apply forwarding and transformation rules at the api gateway level
  


#### cicd

- introduction
  - code commit
    - no longer accept the new customers
    - version control
    - collaboration
    - security
      - authentication
      - authorization
      - encryption
      - cross-account access
  - code pipeline
    - orchesrtate your cicd
    - source -- build -- test -- deploy -- load testing -- ...
    - artifacts:
    - troubleshooting
      - for pipeline/action/stage execution stage changes
      - use eventbridge to create events
      - if pipeline fails, check the console
      - if pipeline cannot perform an action, check iam permission
      - cloudtrail can be used to audit 
  - code build
    - fully managed ci service
    - leverage docker under the hood
    - source: codecommit, github, s3,...
    - build instructions: buildspec.yml
      - env:
        - variables
        - parameter store
        - secrets manager
      - phases
        - install
        - pre-build
        - build
        - post-build
      - artifacts: send to s3 bucket used by next stage
      - cache: files to cache for future build speedup
    - output logs can be stored in s3 or cloudwatch logs
    - use eventbridge to detect failed builds
    - use cloudwatch alarms to notify
  - code deploy
    - deploy applications to
      - ec2, on-prem:
        - perform in-place deployment or blue/green
        - for ec2 and on-prem, codedeploy agent
        - blue/green:
          - new asg
        - deployment speed:
          - all at once
          - half at a time: in-place deployment
          - one a time
          - custom
      - lambda
        - can help do traffic splitting for lambda alias
        - integrated with sam
        - linear
        - canary
        - all at once
      - ecs
        - only blue/green
        - new target group
        - linear
        - canary
        - all at once
      - deployment to ec2
        - appspec.yml + deployment strategy
        - will do in-place deployment
      - deployment to asg
        - in place
        - blue/green: must use ELB
    - redeploy & rollbacks
      - when rollback happens, a new deployment will be created
    - auto rollback
    - gradual deployment control
    - use appspec.yml
  - code star
    - an integrated solution of github, code commit, code build, code deploy, cloudformation, code pipeline, cloudwatch,...
  - code artifact
    - cost-efective artifact management(dependencies) for software development
    - resource policy
  - code guru
    - ml-powered service that automate code review and application performance recommendations
    - codeguru reviewer:
      - support java and python
    - codeguru profiler:
      - help identify your application performance and provides heap summary
      - agent configuration:
        - maxStackDepth
        - MemoryUsageLimitPercent
        - MinimumTimeForReportingInMilliseconds
        - ReportingIntervalInMilliseconds
        - SamplingIntervalInMilliseconds
  - cloud9
    - cloud-based development environment
- continuous integration (ci)
  - push code
  - test/build
  - get feedback
- continuous delivery (cd)
  - ensure deployments happen often and quick



#### serverless application model

- serverless application model
- using yml files to create cloudformaiton templates
- use codedeploy
- recipe
  - transform header: indicates it is sam template
  - quickly sync local changes to aws lambda: sam sync -- watch
  - package & deploy: sam deploy
- sam accelerate(sam sync)
  - synchronize the sam templates to aws
  - synchronize code changes to aws without updating infra (bypass cloudformation)
- sam -- cli debugging
  - sam cli
  - aws toolkit
  - provide lambda-like execution environment locally
- sam policy templates
  - list of templates to apply permissions to your lambda functions
- sam and codedeploy
  - natively uses code deploy to update lambda
  - traffic shifting feature
  - pre and post traffic hooks
  - easy and auto rollback using cloudwatch alarms
  - **note**:
    - AutoPublishAlias: create new versions and point the lambda alias to the latest version
    - DeploymentPreference
    - Alarms
    - Hooks
- sam -- local capabilities
  - start lambda locally
  - invoke lambda locally
  - start an api gateway endpoint
  - generate aws events for lambda functions
- sam -- multi environments
  - in samconfig.toml, deploy resources in multi environments



#### cdk

- overview:
  - using high level components called constructs
  - compared to sam: can provision all aws services
- to use sam to test cdk apps, should run cdk synth
- cdk constructs
  - components including everything cdk needs to create a final cloudformation stack
  - aws construct library:
    - contains 3 different levels of constructs
  - construct hub
    - from aws, 3rd parties and open-source
- constructs -- layer1
  - pure cloudformation resources
  - names start with Cfn
- constructs -- layer2
  - similar functionality but with convinent defaults and boilerplate
  - provide methods to make it simpler to work with
- construct -- layer3
  - called patterns, which represents multi resoures
  - less customization
  - common tasks
- cdk -- bootstrapping
  - The process of provisioning resources for CDK before you can deploy CDK apps into an AWS environment
  - cdktoolkit stack: s3 bucket, iam roles
- cdk -- testing
  - To test CDK apps, use `CDK Assertions Module` combined with popular test frameworks such as Jest (JavaScript) or Pytest (Python)
  - two types of tests:
    - fine-grained assertions(common)
    - snapshot tests
  - import a template
    - Template.fromStack(MyStack) : stack built in CDK
    - Template.fromString(mystring) : stack build outside CDK

  

#### cognito

- overview:
  - cognito user pool
    - authentication
    - serverless data for users for web, mobile apps
    - mfa
    - email & phone
    - password reset
    - federated identities
    - block users if their credentials are compromissed
    - integrations:
      - api gateway
      - ALB
    - triggers
    - hosted authentication UI can be added to apps
    - hosted ui custom domain
      - for custom domains, must create acm certificate in us-east-1
      - custom domain must be defined in the `app integration` section
    - adaptive authentication
      - block sign-ins or require mfa if suspicious
      - examie each sign-in, generate a risk score
      - integrated with cloudwatch logs
    - decoding ID token
      - the token signature must be verified to ensure jwt can be trusted
      - libraries can help you verify the token
      - the payload contains the user info
    - ALB -- authenticate users
      - must use an https listener to set authenticate-oidc or cognito
      - from 3rd identity providers
      - or cognito user pools
      - auth through cognito user pools
      - oidc auth
      - idp (oidc compliant)
  - cognito identity pool (federated identity)
    - get identities for users so they can obtain temoprary aws credentials
    - users can access aws services directly or through api gateway
    - your identity pool can include:
      - public providers
      - cognito user pool
      - oidc or saml
      - allow guess access
    - iam roles
      - default for authenticated and guest users
      - define rules to choose role for each user based on their ID
      - can partition users access using `policy variables`
      - roles must have a trust policy of cognito identity pool
- cognito user pool vs cognito identity pool
  - cognito user pool: authentication
  - cognito identity pool: authorization

    

#### other serverless

- step functions
  - model your workflow as state machines
  - written in json
  - visualization
  - start workflow with sdk, api gateway, eventbridge
- step function -- task states
  - do some work in your state machine
  - can invoke one aws service
  - run one activity
- step function -- states
  - choice state -- branch/condition
  - fail or success state -- stop execution with failure/success
  - pass state -- pass input to output or inject some fixed data without performing work
  - wait state -- provide a delay or until a specific time
  - map state -- dynamic iterate steps
  - parallel state -- begin parallel branches of execution
- step function -- error handling
  - use retry
  - use catch
  - pre-defined error codes:
    - states.all
    - states.timeout
    - states.taskfailed
    - states.permissions
  - state may report its own errors
- step function -- retry (task or parallel state)
  - evaluate from top to bottom
  - errorEquals
  - intervalSeconds
  - backoffRate
  - maxAttempts
    - catch when max attempts are reached
- step function -- catch (task or parallel state)
  - evaluate from top to bottom
  - errorEquals
  - next
  - resultPath
- step function -- resultPath
  - include error in the input
- step function -- wait for task token
  - allow to pause state machine until a task token is returned
  - task can wait for other aws services, human approval, 3rd party integration, call legacy system....
  - append `.waitForTaskToken` to the `resource` field to tell step functions to wait for the task token
  - sendTaskSuccess or sendTaskFailure 
- step function -- activity tasks
  - task work polled and performed by activity worker
  - the worker can run on ec2, lambda, mobile device,...
  - once finish the work, send a response back using sendTaskSuccess or sendTaskFailure
  - to keep the task active:
    - how long a task can wait by setting timeout
    - periodically send a heartbeat
    - can wait up to 1 year
- step function -- standard vs express
- aws appsync
  - aws managed graphql service
  - can combine data from multiple sources
  - work with websocket or mqtt
  - all starts with uploading one graphql schema2
- appsync security
  - api key
  - iam
  - oicd
  - cognito user pool
- aws amplify
  - amplify studio
  - amplify cli
  - amplify libraries
  - amplify hosting
    - built-in cicd
    - pull request preview
    - using cloudfront
    - e2e testing in test phase of cicd, integrated with cypress



#### advanced identity

- aws sts:
  - can return a temporary credentials for roles, saml, idp, cognito
  - assumeRole (usually for cross-account)
  - with mfa
    - using iam conditions: aws:MultiFactorAuthPresent:true
- iam best practices -- general
  - do not use root credentials
  - grant least privilege for groups, users, roles
  - do not store iam credentials on any machine, instead, call sts to obtain temporary credentials
- iam best practices -- iam roles
  - each services should have their own roles
- iam best practices -- cross-account access
- advanced iam -- authorization model evaluation of policies
  - explicit deny
  - explicit allow
  - else deny
- iam policies & s3 bucket policies
  - attached to users, groups, roles
  - bucket policies attached to s3 buckets
  - iam policy + s3 bucket policy = final policy evaluated
- dynamic policies with iam
  - to assign each user to their own path in the s3 bucket:
    - one policy per user, does not scale!
    - create a dynamic policy:
      - `arn::aws::s3:::my-company/home/${aws:username}/*`
- inline vs managed policies
  - aws managed policy
  - custom managed policy
  - inline: strict one-to-one relationship
- iam:PassRole
  - often comes with iam:GetRole
- trust policy: define who can assume the role
- microsoft AD
  - database of objects:
    - user accounts
    - computers
    - printers
    - file shares
    - ...
- aws directory services
  - aws managed microsoft AD: bi-directional trusts
  - AD connector: a proxy to the on-prem AD
  - simple AD: AD compatible managed directory, standalone



#### security and encryption

- kms
  - able to audit kms key usage using cloudtrail
  - key types:
    - symmetric (AES-256)
    - asymmetric (RSA & ECC key pairs)
- aws kms key
  - kms key types:
    - aws owned keys
    - aws managed key:
    - customer managed keys
  - auto key rotation
    - aws kms key: every 1 year
    - custom kms key: must enable auto rotation & on-demand
    - imported key: manual rotation
- copying snapshots across regions
  - remember to attach a key policy to allow cross-account access
- key policies
  - similar to s3 bucket policy
  - default kms key policy
  - custom kms key policy
- kms envelope encryption (needs a CMK to encrypt/decrypt data key(which is used to encrypt/decrypt the big file))
  - limit: 4kb
  - for files > 4kb, we need to use envelope encryption
  - the api we gonna use GenerateDataKey api
  - use data key to encrypt or decrypt big size data or file
- aws encryption sdk
  - implemented envelope encryption for us
  - feature: data key caching
    - can re-use the data key
- kms request quotas
  - when get throttled, use exponential backoff
  - soft limit, send a ticket
- s3 bucket key for sse-kms encryption
- key policy
  - priincipal options
    - iam users
    - federated user sessions
    - aws services
    - all principals
- cloudHSM
  - aws provisioned encryption hardware
  - customers manage the keys themselves
  - good for sse-c
  - HA
  - integrated with kms: configure kms custom key store with cloudHSM
- ssm parameter store
  - integration with cloudformation
  - versioning
  - iam security
  - optional seamless encryption using kms
  - hierarchy
  - standard vs advanced parameter tier
    - can create parameter policies to assign TTL to parameters
- aws secret manager
  - mostly for RDS integration
  - integration with cloudformation
  - can force rotation
  - using kms
  - multi-region secrets
    - read replica in sync with primary secret
    - able to promote a read replica to a standalone
- cloudformation dynamic references
  - ssm parameter store 
  - ssm parameter store secure string
  - secrets manager
  - use cases:
    - ManageMasterUserPassword: create admin secret implicitly for RDS
    - dynamic reference: create a secret, link the secret and rds db togetther
- cloudwatch logs -- encryption
  - using kms
  - at log group level, with a CMK
  - must use api, cannot do it through console
- codeBuild security
  - to access resources in your vpc, make sure specify vpc configure for codeBuild
  - use environment variables
- aws nitro enclaves
  - process sensitive data in an isolated compute environment
  - fully isolated virtual machine
  - use cases: securing private keys, processing credit cards, secure multi-party computation,...




#### other services

- aws ses
  - fully managed
  - allow inbound/outbound emails
  - reputation dashboard, anti-spam feedback, performance insights
  - Supports DomainKeys Identified Mail (DKIM) and Sender Policy Framework (SPF)
  - Flexible IP deployment: shared, dedicated, and customer-owned IPs
  - Send emails using your application using AWS Console, APIs, or SMTP
  - Use cases: transactional, marketing and bulk email communications
- aws opensearch service
  - successor to aws elasticsearch
  - with opensearch, we can search any field, or partially matches
  - complement to other databases like dynamodb
  - managed cluster or serverless cluster
  - does not natively support SQL (via a plugin)
  - come with dahsboard
  - integrated with kdf, iot, cloudwatch logs
  - kds(real time with a lambda) or kdf(near real time with a lambda doing data transformation)
  - cloudwatch logs: subscription filter + lambda/kdf
- aws athena
  - serverless query service --> s3
  - using standard sql
  - used with quicksight for data visualization
  - performance improvement:
    - columnar data(less scan)
    - compress data for smaller retrievals
    - use larger file(> 128mb) to minimize overhead
    - partition datasets
  - federated query
    - use `data source connector` running on aws lambda
- aws managed streaming for apache kafka
  - alternative to aws kinesis
  - fully managed kafka on aws
    - allow to create, update, delete clusters
  - msk serverless
    - without mananing the capacity
  - aws kds vs aws msk
    - kds:
      - 1mb message size
      - data stream with shards
      - shard splitting & merging
      - encryption in-transit and at rest
    - msk:
      - 1mb default, up to 10mb message size
      - kafka topics with partitions
      - can only add partition to a topic
      - encryption in-transit and at rest
    - consumers:
      - kda
      - aws glue for ETL
      - lambda
      - ec2, ecs,eks
- aws acm
  - ssl/tls certificates
  - auto tls renewal
  - integrated with
    - EB
    - cloudfront distro
    - api gateway
    - alb     
- aws private certificate authority (CA)
  - managed service to create private CA (root, subordinaries)
  - issue x.509 certificates
  - certificates only trusted within your organization(not public)
  - work for the services that are integrated with acm
  - use cases:
    - authenticate users, computers, api endpoints, iot
    - enterprises build public key infra
    - encrypt tls communication, cryptographically signing code
- aws macie
  - detect PII or users sensitive data in s3
- aws appConfig
  - configure, validate, and deploy dynamic configurations
    - no need to restart the application
  - used with apps on ec2, lambda, ecs, eks,...
  - gradually deploy changes and rollback if need
- cloudwatch evidently
  - Safely validate new features by serving them to a specified % of your users
  - Launches (= feature flags): enable and disable features for a subset of users
  - Experiments (= A/B testing): compare multiple versions of the same feature
  - Overrides: pre-define a variation for a specific user
  - Store evaluation events in CloudWatch Logs or S3


### lecture hands on


### practice tests

#### table of contents
- [practice exam](#practice-exam)
- [practice tests](#practice-tests)



#### practice exam:
  - 1st try: 61% (40/65)
    - api gateway integration: Mock -- api gateway will return a response without sending the request further to the backend.
    - The simplest way to set up connections to AWS CodeCommit repositories is to configure Git credentials for CodeCommit in the IAM console, and then use those credentials for HTTPS connections.
    - for codebuild to encryption the artifact, use Aws KMS key
    - fan-out: sns + sqs (sns message filtering make sure which message goes to which sqs queue)
    - lambda alias: traffic splitting
    - rds mysql and rds postgresql as well as aurora are the engines that allow to authenticate using aws iam (**note**: no sql server, oracle)
    - ApproximateNumberOfMessagesVisible: sqs queue metric
    - for the api gateway promotion, it can be done either by redeployment to the prod stage or updating a stage variable value
    - A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service.
    - You can choose to have SQS encrypt messages stored in both Standard and FIFO queues using an encryption key provided by AWS Key Management Service (KMS).
    - deployment in EB:
      - immutable
      - traffic splitting
      - both will launch a new set of instances with new version in a new asg
      - these deployment strategy would cause ec2 instances burst balances lost
    - s3 bucket CORSConfiguration
    - s3 bucket policy
    - sqs: purgeQueue and deleteQueue
    - ELB: route more traffic to one instance or one AZ than others, reasons:
      - sticky sessions enabled
      - the instances with higher capacity may receive more traffic, for example, CLB may route more traffic to the higher-capacity instances. so it is recommended to use similar types and configurations to reduce the capacity gaps and traffic imbalances
    - NLB: can handle millions of requests per second, it will open tcp connection to the selected target on the port specified in the listener configuration. and the incoming connections remain unmodified, so the application no need to check x-forwarded-for to see the clients ip addresses     
    - rds multi-az:
      - auto failover
      - os updates: first on standby, and promote it to primary, and then update the old primary, which become the new standby
    - AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications.
    - sqs: message max size: 256kb
    - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration.
    - when encountered a throttled issue, using exponential backoff
    - ec2 user data:
      - run only once when the instance is started
      - run with the root user privileges
    - cloudwatch custom metrics:
      - can publish high-resolution custom metric, and then retrieve it with a certain period of time
      - **note**: the api call `putMerticData` for custom metric is charged
    - tasks on aws step functions
      - task: single unit
    - cloudfront:
      - one cloudfront distro can serve different types of requests from multiple origins
      - for origin group: for failover, primary origin and secondary origin
    - EB: create multiple environment for different application versions
    - for sqs queue:
      - tags are case-sensitive
      - delaySeconds: delay to deliver messages
      - messageRetentionPeriod: message retention
    - EBS works with aws kms:
      - both root and data volumes can be encrypted
      - at rest or in-transit
    - http 503: service unavailable
    - http 504: gateway timeout
    - http 500: internal server error
    - http 502: bad gateway
    - for lambda function:
      - if it is CPU-, network- or memory-bound, then changing the memory setting can dramatically improve its performance.
    - for cloudformation:
      - the condition can be used with parameter secion
    - s3 bucket: cross-account sharing:
      - have to make sure both accounts are within a single partition, for example, us-west-1 (standard aws partition) cannot share with account in china in (aws-cn partition)
    - for cloudfront distribution signed-url:
      - each signer must have a public-private key pair, the signer use private key to sign the url or cookies, then cloudfront use public key to verify it
      - when using root user to manage cloudfront keu pairs, one can only have up to 2 active key pairs per account
      - when using cloudfront key groups, we can have more key pairs associated with a single cloudfront distro
      - the key pair can only created using root user     
    - for ec2 instance:
      - to configure attribute `DeleteOnTermination`, using cli
    - EFS: file storage service
    - s3: object storage service
    - for cloudwatch log:
      - can send logs data from your log groups inot s3 bucket
    - rds: auto-scaling
    - for ecs:
      - when a container instance is in a state of stopped
      - trying to terminate it is not going to remove it from the cluster
      - instead, use ecs console or cli to deregister this container instance, then it will be gone from the ecs cluster
    - for EB rolling update:
      - if some batches are succeeded, while others are failed
      - then some instances run new version, while others run old version
    - cloudfront:
      - can configure https between viewers and cloudfront as well as cloudfront and custom origins(match viewer:configure forward requests using viewers protocols or https only)
    - when configuring ECS:
      - ecs.config file to configure ecs cluster name
      - meaning just change the cluster name for cloudformation template is not enough
      - or container instances will be launched in multi clusters
    - aws serverless application repository (SAR): a managed repository for serverless applications. It enables teams, organizations, and individual developers to store and share reusable applications, and easily assemble and deploy serverless architectures in powerful new ways. 

#### practice tests
  - test#1:
    - 1st try: 81%
  - test#2:
    - 1st try: 76%
  - test#3:
    - 1st try: 67%
  - test#4:
    - 1st try: 72%
  - test#5:
    - 1st try: 73%
  - test#6:
    - 1st try: 76%

### review lectures and tests
#### table of contents
- [test#1](#test#1)
- [test#2](#test#2)
- [test#3](#test#3)
- [test#4](#test#4)
- [test#5](#test#5)
- [test#6](#test#6)

#### review tests
  - #### test#1
    - EB deployment:
      - immutable: additional asg
      - additional batch
      - rolling
      - all at once
      - traffic splitting
      - blue/green (dns change)
    - for the sqs metric: ApproximateNumberOfMessagesVisible
      - not good for target tracking for auto-scaling, because:
      - the number of messages in the queue might not change proportionally to the size of the auto scaling group that process messages. Because the instance has multiple factors that could influence the total number of instances needed.
      - so, the better way is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain
      - to illustrate with an example, let's say that the current ApproximateNumberOfMessages is 1500 and the fleet's running capacity is 10. If the average processing time is 0.1 seconds for each message and the longest acceptable latency is 10 seconds, then the acceptable backlog per instance is 10 / 0.1, which equals 100. This means that 100 is the target value for your target tracking policy. If the backlog per instance is currently at 150 (1500 / 10), your fleet scales out, and it scales out by five instances to maintain proportion to the target value.
      - Step Scaling scales your cluster on various lengths of steps based on different ranges of thresholds. Target tracking on the other hand intelligently picks the smart lengths needed for the given configuration.
      - AWS recommends using target tracking scaling policies instead.
    - for api gateway usage plan:
      - A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key.
      - You can configure usage plans and API keys to allow customers to access selected APIs at agreed-upon request rates and quotas that meet their business requirements and budget constraints.
    - for cloudformation output section:
      - You can use the Export Output Values to export the name of the resource output for a cross-stack reference.
    - for api gateway performance:
      - response caching
      - payload compression
    - for ec2 burstable instances:
      - t2, t3
      - for each new aws account, it is free to use t2.micro within certain usage limits
    - for aws cdk:
      - can build the app before `cdk synth`
      - usually, the toolkit will do it for you, so you cannot forget
    - for ec2 auto scaling:
      - region scoped
      - try to evenly distribute ec2 across AZs
      - in a vpc, clients can choose which subnets for their ec2 to launch
    - when using cloudformation:
      - pseudo parameters in aws:
        - account id
        - etc
    - for codeBuild
      - can configure TIMEOUT setting
    - for api gateway
      - use lambda authorizer to enable 3rd party authorization mechanism: uses bearer token authentication strategies
    - for ELB:
      - You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port.
      - and also the elb security group is allowed by ec2 instances
    - for ALB access log:
      - can be used to analyze traffic patterns and troubleshoot issues
      - has to enable manaually
    - trust policy:
      - the only resource-based policy supported by iam
    - for ACL:
      - used to control which principals in another account can access your resources
    - for SCP
      - specify the max permissions for an organization or an OU
    - for permission boundary:
      - for iam users or roles
      - set max permissions that an identity-based policy can grant to an iam entity
    - aws serverless application repository:
      - pre-built serverless applications
    - for kinesis data stream throttling:
      - use exponential backoff
      - or decrease the frequency or size of your requests
      - increase the shards
      - distribute read and write operations as evenly as possible
    - for aws budget forecast:
      - AWS requires approximately 5 weeks of usage data to generate budget forecasts. If you set a budget to alert based on a forecasted amount, this budget alert isn't triggered until you have enough historical usage information.
    - for iam access analyzer
      - simplifies inspecting unused access to guide you toward least privilege.
    - for codeDeploy:
      - ec2
      - lambda
      - on-prem
      - can do blue/green
    - for SAM:
      - provides shorthand syntax for building serverless applications
      - lambda functions
      - apis
      - databases
      - event source mappings
    - for codeCommit:
      - iam username and password cannot be used to access it
    - for kms:
      - customer-managed cmk
      - aws managed cmk
      - aws owned cmk
    - for efs volumes
      - can provides a file system with your ecs tasks
      - support fargate
    - for secrets manager
      - manage, rotate, retrieve db credentials
      - mainly for rds, redshift, and documentDB
    - for x-ray:
      - can be used to collect data across aws accounts
      - provides e2s view of requests as they travel through your application, showing a map of underlying components
    - for EB:
      - when to add configuration files
      - use: .ebextensions/<mysettings>.config
    - to deploy SSL/TLS server certificates:
      - use ACM
      - use iam: can be used as a certificate manager only when you must support https connections in a region that is not supported by ACM
        - can store secret key
        - can deploy server certificates
        - but have to obtain certificates from outside
        - cannot upload acm certificates
        - cannot manage certificates via iam console
    - for iam policy conditions
      - StringNotEquals
      - Null 
    - ELB: cross-zone load balancing
    - for lambda alias:
      - traffic splitting
    - IAM policy variables:
      - Policy variables act as placeholders. When you make a request to AWS, the placeholder is replaced by a value from the request when the policy is evaluated.
    - for elasticache
      - they are not retional db, cannot be used to run sql queries
    - cloudfront key pairs:
      - can only be created by root user
    - for x-ray:
      - sampling: Sampling rules tell the X-Ray SDK how many requests to record for a set of criteria.
    - for EB:
      - ElastiCache defined in .ebextensions/ - Any resources created as part of your .ebextensions is part of your Elastic Beanstalk template and will get deleted if the environment is terminated.
      - To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch.
    - for cloudformation stackset:
      - create, update, delete stacks across multiple accounts and aws regions
    - for api gateway mapping templates:
      - API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.
    - to enable iam users to access billing and cost management console:
      - configure iam policy
      - use root user account to activate iam user access to the billing and cost management console
    - for SAM supported resources type:
      - api
      - application
      - function
      - httpapi
      - layerversion
      - simpletable
      - statemachine
    - iam access advisor:
      - To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request.
    - iam access analyzer:
      - AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.
  - #### test#2
    - for ec2 asg:
      - one scaling activity used to terminate a unhealthy instance
      - another scaling activity used to launch a new instance
    - Cognito sync:
      - Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.   
    - for parameter in cloudformation:
      - string
      - number
      - commaDelimitedList -- list of strings separated by commas
      - list<number / ec2 vpc id / ec2 sg id / ec2 subnet id>
      - ec2 key pair / ec2 vpc id / ec2 sg id / ec2 subnet id
    - for dynamodb:
      - updateitem action: can edit existing items or add new items if they do not exist
      - in other words, there is no need for putItem action permission in some use cases       - to re-use ssh key:
      - create key pair
      - import the public key into new regions manually
    - codeDeploy lifecycle hooks:
      - ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.
    - for reserved instnace billing:
      - you can run multiple instances concurrently, but only one can receive the benefit of the reserved instance discount
    - for kms envelope encryption:
      - While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits.
      - AWS Lambda environment variables can have a maximum size of 4 KB. Additionally, the direct 'Encrypt' API of KMS also has an upper limit of 4 KB for the data payload. To encrypt 1 MB, you need to use the Encryption SDK and pack the encrypted file with the lambda function.
    - for dynamodb transactionWriteItem operation:
      - group up to 25 write actions(distinct items in one or more tables)
      - optionally include a client token to ensure the request is idempotent(for some reason like connection timeout or network issues, same operations are processed for multiple times)
    - for step functions:
      - AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications.
      - for each step, it can invoke a lambda or a container task or publish a message to a queue when the whole workflow is completed
    - for s3 bucket:
      - all objects are private by default.
      - However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.
    - for lambda provisioned capacity:
      - help lower latency by removing the init time
    - for sns topic:
      - By default, an Amazon SNS topic subscriber receives every message that's published to the topic.
      - To receive only a subset of the messages, a subscriber must assign a filter policy to the topic subscription. A filter policy is a JSON object containing properties that define which messages the subscriber receives. 
    - aws sts is not supported by api gateway
    - aws sqs:
      - no limit for storing messages
      - but in-flight messages do have limits, make sure delete those processed messages
    - cloudfront key pair:
      - use root user to manage key pairs, can only have up to 2 active key pairs
      - the public key is with cloudfront, use private key to sign url or cookie
    - for s3 access logging:
      - do not point access logging to the original bucket
    - for SAM
      - Presence of Transform section indicates it is a Serverless Application Model (SAM) template
    - for EB
      - Include config files in .ebextensions/ at the root of your source code
      - The option_settings section of a configuration file defines values for configuration options. Configuration options let you configure your Elastic Beanstalk environment, the AWS resources in it, and the software that runs your application. Configuration files are only one of several ways to set configuration options.
    - for cloudfront signed url and signed cookie
      - Signed URLs take precedence over signed cookies.
      - signed cookies: multiple files, no url changes
    - for ebs volume:
      - gp2 performance is tied to the volume size
      - 5.3TiB
    - reserved capacity:
      - zonal reserved instances provides reserved capacity
      - regional reserved instances does not provide reserved capacity
    - cognito identity pool:
      - Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers.
    - codeDeploy:
      - ec2, on-prem, lambda, ecs
    - rds and dynamodb:
      - both support transaction operations
    - for dynamodb backup and download:
      - on-demand
      - PITR
      - **note**: the underlying s3 bucket is not accessible
      - use aws glue to copy your table to s3 and download locally and can use in another service, like athena
      - use aws data pipeline(using aws emr under the hood) to create backup and export to s3 and then download: the easiest method using the lowest amount of aws resources(emr and hive details are abstracted)
      - use aws emr + Hive to export your data to s3 bucet and download (if you are aws EMR user)
    - codeDeploy:
      - appspec.yml in the root directory
    - for the sink types supported by kinesis firehose
      - **note**: elasticache is not supported destination for kinesis data firehose
      - elasticSearch (opensearch)
      - s3
      - redshift with s3
    - for secrets manager:
      - You can also use caching with Secrets Manager to significantly improve the availability and latency of applications.
    - for io1 ebs volume:
      - the ratio of iops and volume size is 50:1
    - for vpc flow logs:
      - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.
      - Flow log data can be published to Amazon CloudWatch Logs or Amazon S3.
    - api gateway:
      - integration with cognito user pool
      - default authorizer
    - for step functions:
      - if a task type has a resultPath, meaning it is a required parameter, then this is not a task, but a pass
    - for dynamodb:
      - support eventually consistent reads and strongly consistent reads
      - when set ConsistentRead = true, then using strongly consistent read, otherwise eventually consistent read
    - for aws cli:
      - the option: --dry-run could use to check whether you have the required permissions for the action, without actually making the requests
    - ebs volumes are az scoped
    - for EB deployment:
      - immutable: rollback is automated
      - for rolling or rolling with additional batch: rollback is manual
    - for lambda function container image:
      - must implement the lambda runtime api
      - lambda provides multi-architecture base image. but, the image you build for your function must target only one of the architecture. lambda does not support multi-architecture container images
    - for sqs:
      - delay queue let you postpone the delivery of new messages to a queue for several seconds
    - for ecs and alb:
      - When you deploy your services using Amazon Elastic Container Service (Amazon ECS), you can use dynamic port mapping to support multiple tasks from a single service on the same container instance. Amazon ECS manages updates to your services by automatically registering and deregistering containers with your target group using the instance ID and port for each container.
  - #### test#3:
    - for elasticache:
      - improve latency and throughput for read-heavy workloads
      - or compute-intensive workloads(like recommendation engine) by allowing you to store the objects that are often read in the cache
    - for elb route more traffic to one instance or az:
      - maybe sticky session
      - maybe the instance capacity
    - codeDeploy:
      - in-place
      - blue/green deployment:
        - lambda
        - ecs
        - ec2/on-prem
    - EB:
      - rolling
      - immutable
      - in-place
      - blue/green (needs route53)
    - elasticache:
      - redis in cluster mode: highly reliable, support transactions
      - memcached: designed for simplicity
    - for ec2 detailed monitoring:
      - use `monitor-instances` option
    - s3 bucket access:
      - iam policy
      - bucket policy
      - ACLs: can grant certain permissions(read,write,...) to certain users for a bucket or object
      - query string authentication: create a signed url
    - for api gateway stages:
      - The promotion can be done by redeploying the API to the prod stage or updating a stage variable value from the stage name of test to that of prod in integration endpoint so that the requests will be forwarded to that url.
    - lambda execution context:
      - used for init sdk client, db connection,etc
      - improve performance
    - iam:
      - iam variables to dynamically specify paths for users (s3 bucket files)
    - for s3 access deny:
      - if iam role has proper permissions
      - if s3 bucket policy has no explicit deny
      - if using sse-kms
      - then could be **kms:GenerateDataKey** action
    - GenerateDataKey api call:
      - Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data
      - to encrypt:
        - GenerateDataKey to get a data key
        - Use the plaintext data key (in the Plaintext field of the response) to encrypt your data outside of AWS KMS. Then erase the plaintext data key from memory.
        - Store the encrypted data key (in the CiphertextBlob field of the response) with the encrypted data.
      - to decrypt:
        - Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.
        - Use the plaintext data key to decrypt data outside of AWS KMS, then erase the plaintext data key from memory.
    - for s3 bucket cross-account sharing within different partition
      - for aws standard partition
      - and aws china partition
      - we cannot use iam roles and resource-based policies to do that
    - for aws access control list:
      - An Access Control List (ACL) is a set of rules that define which AWS accounts or predefined groups can access your AWS resources, such as Amazon S3 buckets and objects.
      - permissions: read, write, read_acp, write_acp, full_control
      - only be able to grant permissions to other aws accounts
      - no explicit deny
      - no conditional permissions
      - best practice: use iam policy
    - for ecs config:
      - in ecs.config file, to configure cluster name
      - otherwise, new instances will be launched in the original cluster as well as the new cluster
    - for rds disaster recovery:
      - cross-region read replicas
      - enable auto backup in multi-az in a single region
    - x-ray:
      - sampling rules
    - for sqs message size:
      - max: 256kb
      - or use sqs extended client (up to 2gb)
    - step functions:
      - express workflows: for high event rates and short duration
      - standard workflows: for long-running, durable, auditable workflows where repeat is expensive or harmful. support human approval
    - sqs:
      - store: unlimited
      - in-flight: 120,000 approximately
    - x-ray:
      - To run the X-Ray daemon locally, on-premises, or on other AWS services, download it, run it, and then give it permission to upload segment documents to X-Ray.
    - for api gateway websocket api:
      - In a WebSocket API, the client and the server can both send messages to each other at any time. 
    - for kms-cmk:
      - you can create, rotate, and disable customer-managed CMKs.
    - kinesis data stream: real-time
    - kinesis data firehose: near real-time
    - aws glue: integration service. used to discover, prepare, and combine data for analytics, ml, app development,etc.
    - kinesis data stream:
      - retention: up to 1 year
      - can run audit applition and billing application
    - kinesis data analytics:
      - used to build sql queries and complicated java apps
      - real-time
      - no retention
    - for lambda function types of errors:
      - Invocation errors occur when the invocation request is rejected before your function receives it.
      - Function errors occur when your function's code or runtime returns an error.
    - ssm parameter store vs environment variables
      - parameter store: secure, hierarchical storage, loaded at the runtime
    - codeBuild:
      - fully managed service
      - scales automatically to meet peak build requests.
    - for lambda environment variables:
      - total size of all environment variables should not exceed 4kb
      - no limit on the number of variables
    - for s3 replication:
      - same-region at bucket level, a shared prefix level, or an object level using object tags
      - cross-region at bucket level, a shared prefix level, or an object level using object tags
      - be aware that the s3 lifecycle actions are not replicated, if need, then have to configure at both buckets
    - for EB:
      - web server tier
      - worker tier
    - for s3 bucket policy:
      - can use a condition to identify if a user has not signed in with MFA for some time using `"NumericGreaterThanIfExists": {"aws:MultiFactorAuthAge": "1800"}`
    - for codePipeline:
      - we can orchestrate the whole process, besides, we can add a manual approval if need
    - for ecs cluster:
      - when an instance is in a state of stopped, instead of terminating it, you should deregister your container instance using ecs console or aws cli
    - for rds or dynamodb:
      - the auto-backup or PITR only has 35 days retention
      - need to create snapshots or create backups using aws backup
    - for db iam authentication:
      - rds mysql
      - rds postgresql
    - for ec2 detailed monitoring:
      - the metric data resolution is 5min
      - for high resolutions, better use custom metric
    - for ssm parameter store:
      - you cannot use resource-based policy
      - for advanced tier, you can configure parameter policy for ttl
    - for secrets manager
      - can use resource-based policy
      - can use iam role
    - for cloudformation parameter section:
      - can define : AllowedValues refers to an array containing the list of values allowed for the parameter
    - for codeDeploy agent on ec2:
      - You can use the `:max_revisions:` option in the agent configuration file to specify the number of application revisions to the archive by entering any positive integer
    - for ebs encryption:
      - data at rest
      - all snapshots
      - all volumes created from the snapshots
      - data moved between the volumes and the instances
    - for s3 bucket owner and object owner:
      - Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket
      - With S3 Object Ownership, any new objects that are written by other accounts with the bucket-owner-full-control canned access control list (ACL) automatically become owned by the bucket owner, who then has full control of the objects.
    - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account.
  
  - #### test#4
    - for kinesis data stream throttling error:
      - check if there is a hot shard or partition key is not distributed enough  
    - for Cognito sync:
      - use it to synchronize user profile data across mobile devices and the web without requiring your own backend.
      - so that your app can read and write data regardless of device connectivity status
    - for s3 bucket:
      - can configure CORSRule in corsConfiguration, which is an xml document
    - for sqs:
      - can enable kms encryption for sqs standard queue or fifo queue 
    - for nlb:
      - work on 4th layer of OSI model, handle millions of requests, using tcp connection to send requests without modification.
      - so to check the client info like IP, no need for x-forwarded-for header
    - for aws security best practice:
      - avoid using user credentials
      - instead, using iam role
    - for client request ephemeral port range:
      - linux: 32768-61000
      - win server 2003: 1025-5000
      - win server 2008: 49152-65535
      - elb: 1024-65535
      - nat gateway: 1024-65535
      - lambda: 1024-65535
    - if a user is not authorized to perform an action, and response is encoded
      - then use AWS STS decode-authorization-message to decode the message
      - note: this user must be granted permissions to use this api call: AWS STS decode-authorization-message
    - for aws code commit and kms:
      - data in aws code commit is encrypted in transit and at rest
      - no need kms
    - for cross-account permissions:
      - account A:
        - a role
        - a trust policy
      - account B:
        - a role to assume the role in account A
    - for code deploy lifecycle:
      - applicationStop
      - downloadBundle
      - beforeInstall
      - applicationStart
      - validateService
      - block traffic
      - allow traffic
    - for sqs message max size: 256KB
    - for cloudfront security:
      - between clients and cloudfront:
        - https only
        - redirect http to https
      - cloudfront and the backend: origin policy
        - https only
        - match viewer
    - for migrating github to aws code commit
      - git credentials: the recommended user type for working with CodeCommit.
      - note: github secure token is specific to github
    - for lambda execution context: reuse across function calls
    - for cloudfront:
      - configure a single distro to serve different types of requests from multiple origins
      - note: an origin group consists of two origins: a primary origin and a secondary origin. this is for auto failover
    - sns-sqs:
      - fanout solution
      - sns has filters to decide which messages go to which queue
    - for dynamodb stream:
      - catch events at item level
    - for sse-c:
      - https is a must
      - a key must be sent over https and every single request
    - for api gateway cache:
      - use the Header Cache-Control: max-age=0 to invalidate the existing cache entry
    - for cloudwatch logs:
      - can export log data from cloudwatch log groups to s3 buckets
    - for s3 bucket:
      - 'x-amz-server-side-encryption': 'aws:kms'
      - 'x-amz-server-side-encryption': 'AES256'
    - for code pipeline source:
      - s3 source bucket
      - code commit
      - github, ...
    - to send data from ec2 to kinesis data stream:
      - using kinesis agent(a stand-alone Java software application that offers an easy way to collect and send data to Kinesis Data Streams. )
      - kinesis producer library: is not for continuously monitoring and sending data. it is a middle man between your app code and kinesis data stream api actions
    - for elasticache for redis:
      - HA and scalable
      - with cluster mode enabled
    - for cloudformation:
      - cloudformation package: pack and upload local artifacts
      - cloudformation deploy: create and execute a changeset
    - elasticache for redis:
      - support: lists, sets, sorted sets, hashes,...
      - encryption at rest and in transit
    - for dynamodb scan operation:
      - parallel scan
      - need to run multiple worker threads or processes in parallel
      - each worker will scan a part of a table
    - for code build:
      - cache dependencies in s3 bucket
    - for dynamodb local secondary index:
      - created when the table is launched
      - cannot be changed afterward
    - for sse-kms:
      - auto rotate
    - for lambda:
      - max ram: 10gb
    - cloudformation:
      - aws pseudo parameters
    - for code deploy:
      - if there is a failure
      - a new deployment of the last known working version of the application is deployed with a new deployment ID
    - iam policy and resource-based policy:
      - resource-based policy can be used for cross-account permissions
    - s3 bucket access logs and cloudtrail:
      - for object-level api access logs
      - the bucket owner needs to be the object owner to get the access logs
      - through the object ACL
      - cloudtrail:
        - always deliver object-level api logs to the requester
        - as well as bucket owner only if the bucket owner has permissions for the same api actions on that object
    - to encrypt cloudwatch logs:
      - Use the AWS CLI associate-kms-key command and specify the KMS key ARN
      - note: cloudwatch logs are always encrypted, it is just you can choose which key to use
    - for elasticache for redis:
      - All the nodes in a Redis cluster must reside in the same region
      - While using Redis with cluster mode enabled
        - you cannot manually promote any of the replica nodes to primary
        - multi-az is required
        - You can only change the structure of a cluster, the node type, and the number of nodes by restoring from a backup.
    - for alb:
      - targets types:
        - instance
        - ip
        - lambda
      - when the target type is IP:
        - you can specify IP addresses from specific CIDR blocks only. You can't specify publicly routable IP addresses.
  
  
  - #### test#5
  - #### test#6

















    





