# aws-dev-associate

## road map
- [table of lecture contents](#table-of-lecture-contents)
- [lecture hands on](#lecture-hands-on)
- [practice tests](#practice-tests)
- [review lectures and tests](#review-lectures-and-tests)

### table of lecture contents
- [aws iam](#aws-iam)
- [ec2](#ec2)
- [ec2 instance storage](#ec2-instance-storage)
- [HA and scalability](#ha-and-scalability)
- [rds and aurora and elasticache](#rds-and-aurora-and-elasticache)
- [route53](#route53)
- [vpc](#vpc)
- [s3](#s3)
- [aws cli sdk and iam roles and policies](#aws-cli-sdk-and-iam-roles-and-policies)
- [s3 advanced](#s3-advanced)
- [s3 security](#s3-security)
- [cloudfront](#cloudfront)
- [containers](#containers)
- [elastic beanstalk](#elastic-beanstalk)
- [cloudformation](#cloudformation)
- [aws integration and messaging](#aws-integration-and-messaging)
- [aws monitoring and troubleshooting and audit](#aws-monitoring-and-troubleshooting-and-audit)
- [lambda](#lambda)
- [dynamodb](#dynamodb)
- [api gateway](#api-gateway)
- [cicd](#cicd)
- [serverless application model](#serverless-application-model)
- [cdk](#cdk)
- [cognito](#cognito)
- [other serverless](#other-serverless)
- [advanced identity](#advanced-identity)
- [security and encryption](#security-and-encryption)
- [other services](#other-services)


#### aws iam

- iam users & groups
  - user can belong to multiple groups or no group
  - groups can not be nested
- **Global service**
- iam permissions:
  - users & groups can be assigned `policies`
  - in aws, apply the least privilege principle
  - users who belong to multiple groups can inherit permissions from multiple groups policies
- iam password policy
  - allow all iam users to change their own passwords
  - prevent password re-use
  - more...
- mfa: password + code
  - mfa software(authy, google authenticator, support mutliple tokens)
  - hardware(universal 2nd factor:U2F security key, support multi-users with a single key)
  - hardware key fob mfa device: gemalto
  - hardware key fob mfa device for aws govcloud(US): surepassID
- access aws
  - aws console: password + mfa
  - cli: access key
  - sdk: access key
- iam roles for services
- iam security tools
  - iam credentials report: account level
  - iam access advisor: user level



#### ec2

- ec2 capabilities:
  - renting ec2 virtual machines
  - storing data: EBS
  - distributing load: ELB
  - scaling services: ASG
- ec2 sizing & configuration
  - windows, macos, linux
  - cpu, ram
  - storage: instance-store, ebs & efs
  - network card: public ip address
  - security group: firewall
  - user data
- ec2 user data: bootstrap script
  - only run once at the instance first start
  - runs with the root user permissions
- instance types
  - general purpose
  - compute optimized: HPC, ML, gaming server, batching workloads
  - memory optimized: databases, in-memory databases, big unstructured data real-time processing
  - storage optimized: OLTP, databases, in-memory databases, data warehouse, distributed file systems
- security groups
  - only contain `allow` rules
  - stateful, incoming traffic --> outgoing traffic
  - can be attached to multi-instances
  - application timeout error--> security group
  - all inbound traffic is blocked by default
- classic ports:
  - 22: ssh, sftp
  - 21: ftp
  - 80: http
  - 443: https
  - 3389: rdp (remote desktop procotol -- login windows instance)
- ssh & ec2 instance connect
- ec2 purchaing options
  - on-demand
  - reserved instances
  - saving plans
  - spot instances
  - dedicated hosts
  - dedicated instance
  - capacity reservations (combined with RI, Saving Plans)


#### ec2 instance storage

- ebs volume
  - network drive
  - az-scoped
  - have a provisioned capacity
  - root volume: delete on termination(by default)
  - snapshots: no need to detach, but recommended
    - snapshot features:
      - archive
      - recycle bin
      - fast snapshot restore
- ami
  - customization of ec2 instances
  - region-scoped
- instance store:
  - better I/O
  - good for buffer, cache
- ebs volume types
  - gp2/gp3
  - io1/io2: support ebs multi-attach (up to 16 ec2)
  - st1: big data, data warehouse,...
  - sc1: infrequently access
  - gp and io can be used as root volumes
- efs:
  - multi-az
  - nfs protocol
  - for linux
  - encryption using kms
- efs performance & storage classes
  - performance mode
  - throughput mode
  - storage classes: storage tiers, availability


#### HA and scalability

- scalability: vertical, horizontal
- availability: goes hand in hand with horizontal scaling, (running system in at least 2 data centers)
- ELB
  - health check: done on a port and a endpoint
  - types:
    - CLB
    - ALB:
      - layer 7;
      - http/2 and websocket;
      - route to different target groups: ec2(or ec2 in asg), ecs, lambda, private ip
      - fixed hostname
      - to see the client ip, use header: `X-Forwarded-For`
    - NLB: layer 4 (udp, tcp); handle millions of requests per requests; has `one static ip per az`; support EIP(helpful for whitelisting ip addresses)
      - target groups: ec2, private ip, ALB,
      - health check: tcp, http, https
    - GWLB: layer 3; use `GENEVE` procotol on port `6081`
      - manage a fleet of 3rd party network virtual appliance in aws
      - firewalls, intrusion detection, deep packet inspection system
      - target groups: ec2, private ip
  - security group
  - sticky sessions: works for clb, alb, nlb
    - application-based cookies: created by targets(custom cookies) or load balancers(application cookies)
    - duration-based cookies: created by load balancers
  - cross-zone load balancing
    - alb: enabled by default
    - nlb: disabled by default, no free
    - clb: disabled
  - ssl/tls
    - can manage certs using acm
    - or upload your own certs
    - https listener:
      - must specify a default cert
      - can optionally add a list of certs for multi domains
      - client can use SNI(server name indication) to specify which hostname to reach
      - alb and nlb: support multi https listeners; use sni
      - clb: support only one ssl, 
  - ssl -- SNI
    - only works for alb and nlb, cloudfront
  - connection draining(deregistration delay): time to complete `in-flight requests` while the instance is de-registering or unhealthy 
- ASG
  - a launch template
  - cloudwatch alarms & scaling
  - scaling policies:
    - dynamic scaling:
      - target tracking
      - simple/step
    - scheduled scaling
    - predictive scaling
  - metrics for scaling: `CPUUtilization`, `RequestCountPerTarget`, `Average Network In / Out`, or custom metrics
  - scaling cooldowns: when a scaling activity happends, you are in the cooldown period(no launch or terminate instances)
    - use Golden AMIs
  - instance refresh:
    - update launch template and re-create all ec2 instances
    - using native feature: `instance refresh`
    - setting minimum healthy percentage
    - specify `warm-up` time
 

#### rds and aurora and elasticache

- RDS: relational db service
  - support multi different db engines, including aws aurora
  - managed service
  - cannot ssh to the underlying instances
  - storage auto scaling:
    - have to set `Maximum Storage Threshold`
  - read replicas:
    - up to 15
    - within az, cross az or cross region
    - replication is `async`
    - apps must update db connection string
    - replicas can be promoted to their own db
    - use case: data analytics
    - network cost: same-region, free; cross-region charged
  - multi-az (Disaster Recovery):
    - replication: `sync`
    - one DNS name
    - not used for scaling
    - just a standby
    - from single-az to multi-az: click `modify`, then a snapshot is created, then a new db is created, then a synchronization is established
  - rds proxy:
    - fully managed db proxy
    - pool and share db connections
    - reduce rds & aurora failover time
    - must be access from vpc (private)
- Aurora:
  - support mysql and postgres
  - HA & read scaling:
    - 6 copies across 3 az:
      - 4/6 needed for write
      - 3/6 needed for read
      - up to 15 read replicas
      - auto failover
      - cross-region replication
  - aurora db cluster:
    - writer endpoint
    - reader endpoint
  - rds & aurora security:
    - at-rest encryption
    - in-flight encryption
    - iam authentication
    - security groups
    - no ssh except on `rds custom`
    - audit logs can be enabled and sent to cloudwatch logs
- elasticache:
  - managed redis:
    - multi-az and auto failover
    - read replicas with HA
    - backup and restore
  - managed memcached:
    - mutli-node
    - no HA
    - no persistant
    - no backup and restore
    - multi-threading
  - cache must have an invalidation strategy to make sure only the most current data is used in there
  - cache implementation considerations:
    - outdated data
    - data changing slowly or rapidly
    - data structured well or not
  - lazy loading/ cache-aside/ lazy population:
    - cons: cache read miss cause 3 round trips; data could be stale 
  - write through -- add or update cache when db is updated:
    - cons: data missed until it is added or updated in the main db (using lazy loading to solve it); a lot of data will never be read since each time db is updated, it will also update the cache
  - cache evictions and TTL
    - occur in three ways:
      - delete explicitly
      - memory is full and it is not recently used
      - a TTL is set
    - use cases: leaderboards; comments; activity stream
    - consider to scale up or out if eviction happens too frequently
  - final words of wisdom:
    - lazy loading works for many cases, especially on the read side
    - write-through is usually combined with lazy loading as targeted for the queries or workloads
    - set a ttl is usually not a bad idea, except when using write-through
    - only cache things that make sense

  
#### route53

- dns terminologies:
  - domain registar
  - dns records
  - zone file: contains dns records
  - name server: resovle dns queries
  - TLD
  - SLD
- route53:
  - 100 SLA
  - health check
  - records:
    - domain/subdomain name
    - record type
    - value
    - routing policy
    - ttl
  - records types
    - A
    - AAAA
    - Alias
    - CNAME
    - NS
  - hosted zones
    - public
    - private
    - $0.5 per month per hosted zone
  - TTL:
    - except for Alias records
    - high ttl or low ttl
  - CNAME vs Alias
    - CNAME: not for TLD
    - Alias: TLD or non-TLD, point to aws resources
  - Alias records:
    - map hostnames to aws resources
    - no ttl
    - TLD or non-TLD
    - targets:
      - **Note**: not for ec2 instances
      - elb
      - cloudfront
      - api gateway
      - EB
      - s3 websites
      - vpc interface endpoint
      - global accelerator
      - route53 records in the same hosted zone
  - health check:
    - http health check only for public resources
    - but can integrate with cloudwatch metrics to monitor private resources
    - automated dns failover
      - monitor an endpoint:
        - around 15 global health checkers
        - can be configured to only check on the first 5120 bytes of the responses
      - calculated health checks (similar to composite alarms)
      - monitor cloudwatch alarms
  - traffic flow:
    - visual editor to manage complex routing decision trees
    - can be saved as traffic flow policy and applied to different route 53 hosted zones
  - routing policies
    - simple: no health check; route traffic to a single resource
    - weighted:
      - dns records must have the same name and type
      - health check
      - all zero, then traffic evenly distributed
      - if zero, then no traffic sent
      - weights no need to be 100 when summed up
    - latency-based:
      - health check (failover feature)
      - helpful when latency is a priority
    - failover(active-passive)
    - geolocation:
      - should have a default record
      - health check
    - geoproximity
      - shift traffic to resources based on **bias**
      - must use **route 53 traffic flow**
    - ip-based routing
      - routing is based on ip addresses
      - provide a list of CIDRs for clients
    - multi-value:
      - health check
      - same record name and type
- route 53 and other dns registar
  - register a domain name and then use route 53 name servers



#### vpc

- overview -- basics:
  - vpc: private network
  - public subnet
  - private subnet
  - route tables
- internet gateway
- nat gateway (or nat instances:self-managed)
- NACL & security group
  - NACL: allow or deny; subnet level; stateless
  - security group: only allow; ec2 instance/ENI; stateful
- vpc flow logs:
  - vpc flow logs, subnet flow logs, ENI flow logs
  - logs can go to s3, KDF, cloudwatch logs
- vpc peering:
  - connect two vpc, privately using aws network
  - must not have overlapping CIDR
  - not transitive
- vpc endpoints:
  - gateway endpoints: s3, dynamodb
  - interface endpoint: for DX, s2s vpn
    - public virtual interface
    - private virtual interface
- s2s vpn & DX
  - s2s vpn: on-prem to aws; use public internet; encrypted
  - DX: take a long time to build; private connection; physical connection; private network



#### s3

- buckets:
  - regional service
  - needs a global unique name
- objects
  - the key is the full path: prefix + object name
  - max object size: 5tb
  - muse use `multi-part upload` if size > 5gb
- s3 security:
  - user-based: iam policies
  - resources-based:
    - bucket policies
    - object access control list -- fine-grained
    - bucket access control list -- less common
  - encryption
- s3 static website hosting:
  - make sure the bucket policy allows public access
  - make sure the bucket name is the same as the record name
- s3 versioning
  - enable at bucket level
- s3 -- replication
  - must enable versioning
  - cross-region replication
  - same-region replication
  - asynchronous copy
  - using `s3 batch replication` to replicate existing objects
  - can replicate `delete marker`
  - there is no chaining replicaiton
- s3 storage classes
  - standard
  - infrequent access:
    - standard-IA
    - one zone-IA
  - glacier storage classes
    - instant retrieval
    - flexible retrieval
    - deep archive
  - intelligent-tiering


#### aws cli sdk and iam roles and policies

- ec2 instance metadata (IMDS)
  - allow ec2 to `learn about themselves` without using an iam role
  - can retrieve the iam role from the metadata, but not the iam policy
- IMDS v2 and IMDS v1
- MFA with CLI
  - must create a temporary session using `sts getsessiontoken` api call
- aws sdk
- aws limits (quotas)
  - api rate limits
  - service quotas (service limits)
- exponential backoff
  - use exponential backoff if you encounter `ThrottlingException`
  - retry logic has been included in aws sdk (not for conditional check)
    - must only implement the retry for 5xx errors and throttling
    - do not implement on 4xx client errors
- aws cli credentials provider chain
  - command line options
  - environment variables
  - cli credentials file -- aws configure: `~/.aws/credentials`, `~/.aws/config`
  - container credentials for ecs tasks
  - instance profile credentials for ec2 instance profiles
- aws sdk default credentials provider chain(java)
  - java system properties
  - environment variables: access_key, access_secret
  - default credential profile file: for example `~/.aws/credentials`
  - ecs container credentials for ecs containers
  - instance profile credentials for ec2 instances
- aws credentials best practices
  - never store aws credentials in your code
  - if working with aws, use iam roles
  - if working outside of aws, use environment variables/ named profiles
- sign aws api requests
  - if use sdk or cli, the requests are signed for us



#### s3 advanced

- moving between storage classes
  - using `lifecycle rules`
- lifecycle rules:
  - transition actions
  - expiration actions
  - can be create for certain prefix or object tags
- s3 analytics -- storage class analysis
  - recommendations for standard and standard IA
  - good first step to put together lifecycle rules
- s3 event notifications
  - sns
  - sqs
  - lambda
  - eventbridge:
    - advanced filtering options with json rules
    - multi-destinations
    - eventbridge capabilities
- s3 performance
  - 3,500 put/copy/post/delete per prefix
  - 5,500 get/head per prefix
  - multi-part upload: must use > 5gb
  - s3 transfer acceleration
  - s3 byte-range fetches
    - parallelize get requests
    - can be configured to only fetch part of the data
- s3 select & glacier select
  - less network transfer
  - can filter rows and columns
  - can retrieve less data using sql
- s3 user-defined object metadata & s3 object tags
  - we cannot search the object metadata or object tags
  - instead, we must use external db as a search index, such as dynamodb


#### s3 security

- object encryption
  - sse-s3: use the key managed and owned by aws s3
    - header: "x-amz-server-side-encryption": "AES256"
  - sse-kms: use aws kms to manage the key
    - header: "x-amz-server-side-encryption": "aws:kms"
    - audit using cloudtrail
    - kms limit
  - sse-c: encryption key is managed by you
    - https is a must
    - the key must be provided every request
    - aws only handle encryption
  - client-side encryption: cse
    - use client libraries like: Amazon S3 Client-Side Encryption Library
    - clients handle encryption and decryption
- encryption in transit
  - ssl/tls
  - bucket policy condition: `aws:SecureTransport:`
- default encryption vs bucket policies
  - **note**: bucket policies are evaluated before default encryption
- s3 cors
  - we need to enable correct cors headers for clients to fetch
- s3 MFA delete
  - only root account can enable/disable MFA delete
  - no need: enable versioning; list deleted versions
  - need: permanently delete version; suspend versioning
- s3 access logs
  - set a target bucket(different) for storing access logs
- pre-signed urls
  - use cli, sdk, console to create pre-signed urls
- s3 access points
  - has its own domain
  - an access point policy
- s3 access points -- vpc origin
  - between the vpc gateway endpoint or interface endpoint and s3 buckets
  - vpc endpoint policy must allow access to the target bucket and access point
- s3 object lambda
  - architecture:
    - client
    - s3 object lambda access point
    - lambda
    - s3 access point
  - use cases:
    - redacting personal info before send them back to the caller
    - converting data formats or resizing images

  
#### cloudfront

- overview:
  - cdn
  - DDoS protection, integrated with aws shield, aws waf
- origins
  - s3 buckets:
    - OAC
  - custom origins:
    - s3 websites
    - alb
    - ec2
    - any http backend
- cloudfront vs s3 cross-region replication
  - cloudfront: good for static content availablt anywhere
  - s3: good for dynamic content available in a few regions
- caching
  - at edge locations
  - identify each object using `cache key`
  - increase the cache hit ratio and decrease requests to the origin
  - invalidation: `CreateInvalidation` api
- cache key:
  - unique identifiers for each object
  - hostname + resource portion of the url
  - can add other elements: http headers, cookies, query strings using `cache policies`
- cache policies:
  - http headers:
    - none
    - whitelist
  - cookies
  - query strings
    - none
    - whitelist
    - inlude all-except
    - all (not good, too many)
  - control TTL, can set by the origin using `Cache-Control` header
  - custom policies support
  - **note**: all http headers, cookies, query strings in `cache key` are included in origin requests
- origin request policy
  - Specify values that you want to include in origin requests without
including them in the Cache Key (no duplicated cached content)
  - can include: http headers, cookies, query strings
  - ability to add cloudfront http headers and custom headers to an origin requst that was not included in the viewer request
  - support custom policy
- cache invalidations
  - we can force an entire or partial cache refresh by performing cloudfront invalidations
  - can specify invalidation path: `*`, `/images/*`
- cache behaviors
  - configure different settings based on the content type or path pattern: `/api/*`, `/*`
  - the default cache behavior is always the last to be process and is always `/*`
- geo restriction
  - restrict who can access your distribution:
    - allowlist
    - blocklist
- signed url / signed cookies (normally after authentication and authorization)
  - for example, only premium users can access paid shared content
  - can attach a policy with:
    - url expiration
    - ip ranges
    - trusted signers (which aws accounts can create signed url)
  - signed url: individual files
  - signed cookies: multi files
- cloudfront signed url vs s3 pre-signed url
  - cloudfront:
    - filter by ip, path, data, expire
    - define access path
    - only root user can manage it
    - utilize caching features
  - s3:
    - uses iam key
    - issue a request as the person who pre-signed the url
- cloudfront signed url process
  - signers:
    - a trusted key group(recommended)
    - an aws account that has a cloudfront key pair(not recommended, should not use root account)
  - can create multi key groups
  - generate key pair
- price classes
  - all
  - 200: most regions, exluding the most expensive ones
  - 100: only the least expensive ones
- multiple origins
  - route to different origins based on the content type
  - based on path pattern: `/images/*`, `/api/*`
- origin groups
  - increate HA and do failover
  - one origin group: one primary and one secondary origin
- field level encryption
  - protect sensitive user info for each request
  - encryption at the edge locations
  - specify fields(max 10) to encrypt and public key used for encryption
  - decryption at the web server side
- real-time logs
  - using KDS
  - choose:
    - sampling rate
    - specify fields and cache behaviors(path patterns)



#### containers

- overview:
  - apps are packaged in containers that can run on any os
  - docker image repositories:
    - docker hub
    - aws ecr
- docker vs virtual machines
  - docker containers are sharing the resources
- docker containers management on aws
  - aws ecs
  - aws eks
  - aws fargate: serverless container platform, working with ecs and eks
  - ecr: storing container images
- aws ecs
  - ec2 launch type
    - each ec2 must have ecs agent to register in the ecs cluster
    - aws takes care of starting / stopping containers
    - ecs tasks: launch ec2 instances on ecs clusters
  - fargate launch type
    - no ec2 instances to manage
    - just take care of task definitions
    - aws runs ecs tasks for your based on cpu / ram you need
    - to scale, just increase the number of tasks
  - iam roles
    - ec2 instance profile: (ec2 launch type only)
      - used by ecs agent
      - make api calls to ecs service
      - send logs to cloudwatch logs
      - pull images from ecr
      - reference data in secrets manager or ssm parameter store
    - ecs task role
      - each task has a role
      - defined in task definition
  - load balancer integrations
    - alb: most use cases
    - nlb: only for high thoughput
    - clb: not recommended
  - data volumes (efs: serverless)
    - mount EFS onto ecs tasks
    - works for ec2 and fargate types
    - **note**: s3 cannot be used as file system
  - ecs auto scaling
    - uses aws application auto scaling
    - based on : cpu, ram, or alb requests count per target
    - target tracking
    - step scaling
    - scheduling schaling
    - ecs auto scaling (task level)
    - ec2 auto scaling (instance level)
  - ec2 launch type -- auto scaling ec2 instances
    - scaling by adding ec2 instances
    - asg
    - ecs cluster capacity provider: paired with asg
  - rolling updates
    - min: minimum healthy percent (relative to the actual running capacity: 100%) 
    - max: maximum percent (actual running capacity + allowed to create tasks: extra)
  - integrated with eventbridge:
    - on-schedule
    - on-demand
  - integrated with sqs
  - task definitions:
    - json form to tell ecs how to run a docker container
    - cpu, ram, iam role, image, port, logging, env, networking
    - up to 10 containers in one task definition
  - load balancing:
    - ec2 launch type:
      - dynamic host port mapping if the container port is defined in the task definition
      - alb will find the right port
      - must allow ec2 security group to any port from alb security group
    - fargate
      - each task has a private ip
      - only container port
      - one iam role per task definition
  - environment variables
    - hardcoded
    - ssm parameter store
    - secrets manager
    - env files(bulk) -- s3
  - data volumes (bind mounts)
    - share data between multi container in the same task definition
    - works for both ec2 and fargate
  - task placement (only for ec2 launch type)
    - task placement contraints
      - distinctInstance: tasks are placed on different instances
      - memberOf: placed on ec2 instances with a specified expression; or uses the cluster query language(advanced)
    - task placement strategies: best effort
      - binpack: aim the least available amount of cpu and ram
      - random
      - spread: tasks are spread evenly based on the specified value: az
      - or mix them up
    - task placement process:
      - identify the proper ec2 instance
      - identify the instance with task placement contraints
      - identify the instance with task placement strategies
      - select the instance
- ecr
  - container images registry
  - integrated with ecs
  - public and private
- aws copilot
  - help run apps on AppRunner, ECS, and fargate
  - help focus on building apps rather than setting up infra
  - provision all required infra for containerized apps
  - auto-deploy using codepipeline
  - deploy to multiple env
  - troubleshooting, logs, health status
- aws eks
  - aws managed kubernates cluster
  - eks supports ec2 and fargate
  - cloud-agnostic
  - collect logs and metrics using cloudwatch container insights
  - Node Types:
    - managed node group: create and manage nodes(ec2 instances) for you
    - self-managed nodes: you create nodes and register to the cluster and then managed by asg
    - aws fargatea
  - data volumes (file systems)
    - need to specify storageClass manifest on your eks cluster
    - leverages a container storage interface compliant driver
    - support:
      - ebs
      - efs (works with fargate)
      - fsx for lustre
      - fsx for netapp ontap



#### elastic beanstalk

- overview:
  - provision underlying resources: elb, asg, ec2, rds,...
  - managed service
  - web server tier & worker tier
  - devs only need to take care of application codes
  - still have full control over the configuration
- components
  - applications
  - application versions
  - environments: each application version
- deployment modes:
  - single instance
  - HA
- deployment options for updates
  - all at once
  - rolling (manual rollback)
  - rolling with addtional batches (manual rollback)
  - immutable: create a new asg with new versions, then swap to the new asg; high cost, long deployment, quick rollback(just terminate the asg, good for prod)
  - blue/green: (not a direct feature, need to work with route53 using weight policy and then swap urls when ready) create a new environment and switch over when ready
  - traffic splitting: (using the alb to do traffic splitting) canary testing -- send small portion of traffic to the new deployment. can trigger an automated rollback(very quick), new instances migrate to the original asg, old verion will be terminated. deployment health is moinitored
- deployment process
  - describe dependencies
  - package code as zip and describe dependencies
  - use console or cli to upload zip and then deploy
- lifecycle policy
  - EB can store up to 1000 app versions
  - to phase out old app versions, use lifecycle policy
    - based on time
    - based on space
  - the in-use version cannot be deleted
  - Option not to delete the source bundle in S3 to prevent data loss
- extensions
  - a zip file containing our code must be deployed to EB
  - all parameters set in UI can be configured with code using files
  - requirements:
    - must be in the path `.ebextensions/` directory in the root of source code
    - yml/json
    - .config extensions (exmaple: logging.config)
    - able to add other aws resources
    - able to modify some default settings using : `option_settings`
- under the hood
  - using cloudformation
  - can define cloudformation resources in your .ebextensions directory
- cloning
  - clone an environment
  - all resources and configuration are preserved
    - data in db is not preserved
- EB migration: load balancer
  - the load balancer type cannot be changed(clb, nlb,alb)
  - create a new environment with different type of load balancer, then swap CNAME or route 53 update
- EB with RDS
  - database lifecycle is tied to the EB environment lifecycle
  - good practice is to create a rds instance separately and provide our environment with the connection string
- EB migration: decouple RDS
  - create a snapshot of db (safeguard)
  - go to rds console to enable deletion protection
  - create a new environment but without rds, instead, just point to the existing rds instance
  - then perform CNAME swap (blue/green) or route53 update
  - terminate the old environment (rds will stay)
  - delete the cloudformation stack(in DELETE_FAILED state, cuz the rds unable to delete)


#### cloudformation

- benefits:
  - infra-as-code: good for version control
  - cost: good for cost management using tags (saving strategy)
  - productivity: quickly recreation
  - separation of concerns: multi stacks
- process:
  - template
  - s3 bucket
  - aws cloudformation
  - create stack to provision resources
- deployment
  - manual
  - automated
- cloudformation -- resources
- cloudformation -- parameters
  - to reference a parameter: using `!Ref`
  - pseudo parameters (provided by aws)
- cloudforamtion -- mappings
  - use `!FindInMap` to return a value from the map
- cloudformation -- outputs
  - declare optional outputs
  - using `Export` section
  - using `!ImportValue` to reference
- cloudformation -- conditions
  - intrinsic functions(logical)
  - can be applied to resources, outputs,... but not `parameters`
- cloudformation -- intrinsic functions
  - !GetAtt: get the attributes of the resources
  - !Base64: use to encode user data for ec2 for example (convert string to base64)
  - condition functions
- rollbacks
  - creation fails
  - update fails
  - rollback fails: manual fix, then call `ContinueUpdateRollback` api to continue rollback
- service role
  - iam users with the iam role which include `iam:PassRole` permission
  - make sure iam users have the least privileges
- cloudformatin capabilities
  - CAPABILITY_NAMED_IAM & CAPABILITY_IAM
  - CAPABILITY_AUTO_EXPAND
  - InsufficientCapabilitiesException
- deletion policy -- for stack resources
  - delete: won't work with s3 bucket if it is empty
  - retain: work with resources
  - snapshot
- stack policy -- for stack updates
  - define update actions that are allowed during stack updates
  - must explicitly allow
- termination protection -- for stack deletes
- custom resources
  - define custom resources
  - have custom logic (scripts) via lambda
  - service token specify where to send the requests(lambda, sns)
  - use case: empty s3 bucket before deleting it
- stacksets
  - create, update, delete stacks across multi accounts and regions
  - can be applied within an aws organization


#### aws integration and messaging

- decouple your applications(scale independently from our applications):
  - sqs: queue model
  - sns: pub/sub model
  - kinesis: real-time streaming model
- sqs:
  - standard queue:
    - unlimited throughput, unlimited number of messages in queue
    - low latency
    - 256kb max size of each message
    - retention: 4 - 14 days
    - best-effort, at least once delivery
  - producing messages
    - sendMessage api call
    - message is persisted until a consumer deletes it
  - consuming messages
    - ec2, lambda,...
    - poll messages (up to 10)
    - process messages
    - delete message using deleteMessage api call
  - multi ec2 consumers
    - poll and process messages in parallel
    - scale consumers horizontally to increase throughput
  - with asg
    - scaling based on an alarm set on the cloudwatch metric: ApproximateNumberOfMessages
  - decouple between application tiers
  - security
    - encryptioin in-transit and at rest
    - or client side encryption
  - queue access policy
    - resource-based policy
  - message visibility timeout
    - a message is invisible when it is polled
    - by default 30 seconds, then after that, it becomes visible again
    - consumer call the ChangeMessageVisibility api    
  - dead letter queue
    - a message will be sent to a dlq after being failed to be processed
    - configure `MaximumReceives`
    - standard queue -- standard dlq queue
    - fifo queue -- fifo dlq queue
    - good to set retention of 14 days in the dlq
    - redrive to source
      - when our code is fixed, we can redrive messages from dlq to the source queue(or any other queue) in batches without writing custom code
  - delay queue
    - delay a message up to 15 mins
  - long polling
    - can optionally wait if there is no message in the queue
    - decrease the number of api calls
    - is preferable to short polling
    - can enable at queue level
  - extended client
    - max size of each message is 256kb
    - to send data like 1GB, use `SQS extended client`(java library)
  - must know api
    - CreateQueue
    - DeleteQueue
    - PurgeQueue
    - SendMessage, ReceiveMessage, DeleteMessage
    - MaxNumberOfMessages (up to 10, for receiving messages api)
    - ReceiveMessageWaitTimeSeconds: long polling
    - ChangeMessageVisibility
    - batch apis for sending, deleting, changing messages visibility: help decrease your costs
  - fifo queue
    - limited throughput: 300 per second, 3000 per second with batch
    - exactly once delivery
    - messages processed in order
    - deduplication
      - interval is 5 min
      - two methods:
        - Content-based deduplication: will do a SHA-256 hash of the message body
        - Explicitly provide a Message Deduplication ID
      - prevent producers from sending duplicated messages
    - message grouping
      - same value for MessageGroupID in fifo queue
      - ordering inside the group is kept
      - ordering across groups is not guaranteed
      - each group has different consumer
- aws SNS
  - pub/sub: send one message to many receivers
  - one event producer can only send messages to one sns topic
  - many event receivers or subscribers can listen to one sns topic
  - Each subscriber to the topic will get all the messages (note: new feature to filter messages)
  - integrated with many aws services as message senders
  - how to publish
    - topic publish using sdk
    - direct publish for mobile apps sdk
  - security: similar to sqs
  - sns + sqs: fan out
    - can add more sqs subscribers over time
  - fifo topic
    - similar to fifo queue
    - message group id (all messages ordered in the same group)
    - deduplication
    - **note**: can have sqs standard or fifo as subscribers
    - limited throughput
  - message filtering
    - subscribers use it to filter messages based on their needs
    - otherwise all message will be received
- aws kinesis
  - collect, process, and analyze streaming data in real-time
  - kinesis data streams
    - retention: 1-365 days
    - support replay
    - data in kinesis is immutable, cannot be deleted
    - data with the same partition key goes to the same shard (ordering)
    - capacity modes
      - provisioned mode:
        - choose number of shards
        - each shard: 1mb in , 2mb out per second
      - on-demand mode:
        - default capacity provisioned: 4mb in or 4000 records per second
    - security
      - using iam policy
      - encryption in-transit and at rest or choose client-side encryption
      - vpc endpoint is available for kinesis
      - monitoring using cloudtrail
    - producers
      - data record: sequence number; partition key, data blob
      - aws sdk, kinesis producer libracy(KCL), kinesis agent
      - putRecord api
      - 1mb or 1000 records per sec per shard
      - use batching with putRecord api
      - use highly distributed partition key to avoid hot partition
      - `ProvisionedThroughputExceeded`
        - use highly distributed partition key
        - increase shards
        - retry with exponential backoff
    - consumers
      - max 5 consumers per shard
      - lambda, kda, kdf, kcl, custom consumer(aws sdk -- classic or enhanced fan-out)
      - custom consumer
        - classic: multi consumers share 2mb out; max 5 getRecord api/sec
        - enhanced fan-out: multi consumers have 2mb out each
      - lambda
        - support classic and enhanced fan-out consumers
        - read records in batches
        - can configure batch size and batch window
        - retry
        - can process up to 10 batches per shard simultaneously
      - KCL
        - java library
        - each shard to be to read by only one kcl instance
        - progress is checkpointed into dynamodb
        - can run on ec2, on-prem, EB
        - track other workers and share the work amongst shards using dynamodb
        - records are read in order at shard level
        - one kcl can read multi shards
    - kinesis operation -- shard splitting
      - used to increase stream capacity (1mb in per shard)
      - divide hot shard
      - no auto scaling, just manual
      - cannot split into more than 2 shards at a time
      - the old shard will be closed and deleted once the data is expired
    - kinesis operation -- merging shards
      - decrease the stream capacity and save costs
      - group two shards with low traffic(cold shards)
      - cannot merge more than 2 shards at a time
      - the old shard will be closed and deleted once the data is expired
  - kinesis data firehose
    - fully-managed, auto scaling, serverless
    - pay for data going into firehose
    - near real time
      - buffer size: minimum 1mb
      - buffer interval: 0 - 900 sec
    - support data multi format, conversions, transforms, and compression
    - support custom data transform using lamdba
    - can send failed data to s3 bucket
    - no data storage
    - no replay
  - kinesis data analytics
    - sql application:
      - real time analytics on kds and kdf using sql
      - add reference data from s3 to enrich streaming data
      - fully-managed, serverless
      - auto scaling
      - pay-as-you-go
      - output: kds, kfd
    - apache flink( java, scala, or sql)
      - source: kds, aws msk
      - no kdf
  - kinesis video streams
  - ordering data into kinesis using partition key
  - ordering data into sqs using fifo queue (no need group ID, unlike sns), with `only one consumer`
    - to add more consumers, use group ID, similar to partition key in kinesis
  - kinesis vs sqs ordering



#### aws monitoring and troubleshooting and audit

- importance of monitoring
  - deploy applications:
    - safely
    - automated
    - infra-as-code
    - leverage aws managed services
  - app users:
    - latency
    - outage
    - communicate with it support
    - troubleshooting and remediation
  - internal monitoring
    - can we prevent issues before they happen
    - performance and cost
    - trends(scaling patterns)
    - learning and improvement
- monitoring in aws
  - aws cloudwatch
    - metrics
    - logs
    - events
    - alarms
  - aws x-ray
    - troubleshooting application performance and errors
    - distributed tracing of microservices
  - aws cloudtrail
    - api calls
    - auditing
- cloudwatch metrics
  - for every aws service
  - metric is a variable
  - belong to namespaces
  - determined by dimensions
  - have timestamps (at second granularity, cloudwatch will aggregate multi data points if they are in the same timestamp)
  - can create custom dashboards
- ec2 detailed monitoring
  - 1 min interval
  - will scale faster when using asg
  - aws free tier allows 10 detailed monitoring metrics
- custom metrics
  - possible to send custom metrics
  - example, memory usage, disk space, number of logged in users,...
  - can use dimensions to segment metrics
  - metric resolution:
    - standard: 1min
    - high resolution: 1/5/10/30 seconds
    - **note**: accept metric data points two weeks in the past and 2 hrs in the future
- cloudwatch logs
  - log groups
  - log stream
  - log expiration
  - cloudwatch logs can be sent to s3, kds, kdf, lambda, opensearch
  - encrypted by default
  - can set kms custom key
  - sources:
    - sdk, cloudwatch agent,
    - EB
    - ecs
    - aws lamdba
    - api gateway
    - cloudtrail
    - route53
- cloudwatch log insights
  - search and analyze log data
  - can query multi log groups in different aws accounts
  - it is a query engine, not a real-time engine
- cloudwatch log -- s3 export
  - can take up to 12 hrs, not near real time
- cloudwatch log subscriptions
  - real-time log events
  - lambda, kdf, kds
  - subscription filter
  - cross-account subscription
- cloudwatch logs aggregation multi-account & multi-region
- cloudwatch logs for ec2
  - need to run cloudwatch agent with iam permission
  - cloudwatch agent can be set on-prem
- cloudwatch logs agent & unified agent
  - unified agent: new version
    - centralized configure using ssm parameter store
    - collect additional system-level metrics
    - by default: disk, cpu, network
- cloudwatch logs metric filter
  - can use filter expression
  - not retroactive
  - able to specify up to 3 dimensions for metric filter
- cloudwatch alarms
  - triggered by any metric
  - period:
    - length of time to evaluate the metric
    - high resolution custom metrics: 10, 30, or multi of 60 sec
  - targets:
    - ec2
    - ec2 auto scaling
    - sns
  - composite alarms
    - monitor multi alarms states
    - help reduce alarm noise
  - ec2 instance recovery:
    - status check
    - recovery: same private, public, EIP, metadata, placement group
  - good to know
    - alarms can be created on cloudwatch logs metric filter
  - synthetics canary
    - aws version of e2e tests
    - integrated with alarms
    - blueprints:
      - heartbeat monitor
      - api canary
      - broken link checker
      - visual monitoring
      - canary recorder
      - GUI workflow builder
- aws eventbridge
  - schedule: cron jobs
  - event patterns
  - trigger lamdba, send sqs, sns,...
  - event buses
    - can be accessed by other aws accounts
    - can archive event
    - can replay event
  - schema registry
    - eventbridge can analyze and infer the schema
    - allow to create custom registry
    - can be versioned
  - resource-based policy
  - multi-account aggregated
- aws x-ray
  - great for distributed system with a plenty of microservices
  - review request behavior
  - check service sla
  - check if throttled
  - integration:
    - lambda
    - EB
    - ecs
    - elb
    - api gateway
    - ec2 or on-prem
  - leverages tracing
    - tracing: e2e way to follow a request
    - each service will add its own `trace`
    - tracing is made of segments (subsegments)
    - annotations: add extra info
    - security: iam, kms
    - able to trace:
      - every request
      - sample request (%, for example 3%)
  - to enable x-ray
    - using aws x-ray sdk
    - each application must have iam rights to write data to x-ray
  - x-ray magic:
    - collect data from different services
    - service map is computed from all segments and traces
    - x-ray is graphical, non-technical people can help
  - troubleshooting:
    - on ec2:
      - check the iam permission
      - check the x-ray daemon
    - on lambda:
      - check iam permission
      - enable lambda x-ray active tracing feature
      - ensure x-ray client initialized in the code
  - intrumentation in the code
    - the measure of product’s performance, diagnose errors, and to write trace information.
    - to instrument your application code, using sdk
    - can custom annotations using interceptors, filters, handlers, middlewares,...
  - concepts:
    - segments
    - subsegments
    - trace
    - sampling: decrease the amount of request sent to x-ray
    - annotations: key-value pairs used to `index` traces with `filters`
    - metadata: not indexed
  - sampling rules:
    - control the amount of data sent to x-ray
    - By default, the X-Ray SDK records the first request `each second`, and `five percent` of any additional requests.
    - `One request per second is the reservoir`, which ensures that at least one trace is recorded each second as long the service is serving requests.
    - `Five percent is the rate` at which additional requests beyond the reservoir size are sampled.
  - custom sampling rules
  - x-ray write apis
  - x-ray read apis
  - x-ray with elastic beanstalk
    - EB platform include x-ray daemon
    - can enable by setting an option via eb console or with a configuration file (in the .ebextensions/xray-daemon.config)
    - remember to check the ec2 iam permission and x-ray sdk
    - **note**: x-ray daemon is not provided for multi-container docker
  - ecs + x-ray
    - x-ray daemon as a separate container
    - x-ray daemon as a sidecar container with the main container
    - for fargate cluster
      - x-ray daemon as a sidecar container
- aws distro for openTelemetry
  - open source version of aws x-ray
  - a standard set of apis to multiple destinations 
- aws cloudtrail
  - enabled by default
  - get a history of events/api calls made within aws account
  - a trail can be applied to all regions or a single region
  - Provides governance, compliance and audit for your AWS Account
  - cloudtrail events
    - management events (enabled by default), can separate `read events` and `write events`
    - data events (not enabled by default due to a large amount of operations)
    - cloudtrail insights events (findings)
  - event retention: 90 days, or choose to send them to s3 bucket
  - integration with eventbridge



#### lambda

- benefits:
  - cheap
  - integrated with a lot aws servcies
  - support multi programming languages
  - increase ram will improve cpu and network
    - once reach 1,792 mb for ram, we have a whole vCPU,
    - after 1,792mb, consider using multi-threading 
- synchronous invocation
  - cli, sdk, alb, api gateway
  - user invoke
  - service invoke
- integrated with alb
  - lambda function must be registered in a target group
- alb + lambda -- permission
  - lambda resource policy
- alb multi-header values
  - When you enable multi-value headers, HTTP headers and query string parameters that are sent with multiple values are shown as arrays within the AWS Lambda event and response objects.
- asynchronous invocations
  - s3, sns, eventbridge, ses, IoT,...
  - events are placed in an event queue
  - make sure the processing is idempotent
- s3 event notifications
  - simple s3 event pattern -- metadata sync
- event source mapping
  - kds
  - sqs, sqs fifo
  - dynamodb stream
  - Common denominator: records need to be polled from the source
  - lambda function is invoked synchronously
- streams and lambda (kinesis and dynamodb)
  - An event source mapping creates an iterator for each shard, processes items in order
  - configure batch size and batch window to lower traffic
  - can also process up to 10 batches in parallel
  - in-order processing is still guaranteed for each partition key
  - error handling
    - by default, one single error will cause whole batch to be reprocessed
    - but can configure to use `partial batch response`
    - discard events can go to a destination
- event source mapping sqs & sqs fifo
  - recommend: set queue visibility timeout to 6x the timeout of your lambda function
  - to use a dlq
  - support fifo queues, scaling up to the number of active message groups
  - Lambda deletes items from the queue after they're processed successfully.
  - Occasionally, the event source mapping might receive the same item from the queue twice, even if no function error occurred.
  - You can configure the source queue to send items to a dead-letter queue if they can't be processed.
- lambda event mapper scaling
  - kinesis data streams and dynamodb stream
    - one lambda invocation per stream shard
    - if in parallel, up to 10 batches processed per shard simultaneously
  - sqs standard
    - add 60 lambda instances per minute to scale
    - up to 1000 batches in parallel
  - sqs fifo
    - group ID
    - scale to the number of active message groups
- lambda event and context object
  - event:
    - contains the data(from calling service) to be processed
  - context:
    - provides info about the invocation, function, and runtime environment
- lambda -- destinations
  - can configure to send result to destination
  - `asynchronous invocations`:
    - sqs
    - sns
    - lambda
    - eventbridge bus
  - event source mapping: for discarded event batches
  - **note**: events can be sent to dlq directly from sqs
- lambda execution role
  - grant lambda to access other aws resources
  - when using event source mapping, lambda uses a role to read event data
- resource-based policies
  - grant lambda function access to other services or aws accounts
- environment variables
  - adjust the function behavior without updating code
- lambda logging & monitoring
  - cloudwatch logs: make sure lambda functions has permission to write logs to cloudwatch logs
  - cloudwatch metrics
- tracing with x-ray
  - enable x-ray active tracing
  - lambda does have a x-ray daemon running under the hood
  - make sure use x-ray sdk and initialize it
  - make sure the lambda has the right permission for x-ray (a managed policy)
- customization at the Edge
  - edge functions:
    - a code attached to the cloudfront distro
  - cloudfront provides two types:
    - cloudfront functions
    - lambda@edge
    - already deployed globally
    - fully managed
    - customize cdn content
    - use cases:
      - website security and privacy
      - seo
      - bot mitigation
      - a/b testing
      - auth
      - ...
  - cloudfront functions
    - lightweight functions
    - for high-scale, latency-sensitive cdn customization
    - viewer request: cloudfront receives the requests from clients
    - viewer response: before cloudfront forwards the response to the viewer
    - a native feature
  - lambda@edge
    - scale to 1000 requests per second
    - used to change cloudfront requests and responses:
      - viewer requests: after cloudfront receives the request from client
      - viewer response: before cloudfront forwards the response to the client
      - origin requests: the request before cloudfront forwards to the origin
      - origin response: after cloudfront receives the response from the origin
    - author your lambda in one aws region, then cloudfront replicate to its locations 
  - cloudfront function vs lambda@edge
    - cloudfront function: cache key normalize; header modify; url rewrite; auth
    - lambda@edge: longer execution time, code depends on a 3rd libraries; able to access external services; file system access
- lambda with VPC
  - by default, a lambda is outside of your vpc, in the aws-owned vpc. thus cannot access private resources
  - then must specify subnet and sg
  - then lambda will create an EIP
  - internet access:
    - deploy in a public subnet does not give it a public ip
    - deploy in a private subnet + nat gateway and internet gateway
- lambda function configuration
  - ram:
    - at 1792mb, one full vCPU
    - after 1792mb, need to use multi-threading in your code
  - if computation heavy, then increase ram
- lambda execution context
  - temporary runtime environment
  - great for database connection, http clients or sdk client initialization
  - will be maintained for some time for other lambda invocation (re-use)
  - the context includes the /tmp directory
- the /tmp directory
  - download a big file
  - need disk to do some task
  - max size 10gb
  - remain when execution context is frozen
- lambda layers
  - externalize dependencies for re-use
- lambda file system mounting
  - if lambda is in a vpc, then it can access efs
  - must use efs access point
  - limit: efs connection limit and connection burst limits
- lambda concurrency and throttling
  - limit: 1000 concur
  - can set `reserved concur` at function level
  - can increase limit by opening a ticket
- cold starts & provisioned concur
  - provisioned: is already allocated before the function gets called. no cold start
- function dependencies
  - the aws sdk comes by default for every lambda invocation, so no need to bundle the aws-sdk
- lambda and cloudformation
  - inline: use Code.ZipFile, cannot have dependencies
  - through s3: must specify the s3 zip location
- lambda container images
  - deploy lambda function as a container image (up to 10gb) to ecr
  - can test the container locally using lambda runtime interface emulator
  - unified workflow to build apps
  - best practices:
    - use aws-provided base image
    - use multi-stage builds
    - build from stable to frequently changing
    - use a single repo for functions with large layers
- lambda versions
  - the $latest
  - versions: immutable, have their own arn
- lambda aliases
  - pointers to lambda versions
  - enable canary deployment
- lambda & codedeploy
  - codedeploy help automate traffic shift for lambda alias
  - integrated with SAM
  - deployment strategies:
    - linear: increase traffic every x min to 100%
    - canary: try x then 100%
    - allAtOnce
  - appspec yml
    - name
    - alias
    - currentversion
    - targetversion
- lambda -- function url
  - can be applied to alias or $latest
  - throttle your function by using reserved concur
  - security:
    - resource-based policy
    - cors
    - authType: configure public access
      - none: allow public access and unauthenticated access
        - resource-based policy is always in effect
      - aws_iam: identity-based policy and resource-based policy are evaluated
        - same account: Identity-based Policy OR Resource-based Policy as ALLOW
        - cross account: Identity-based Policy AND Resource Based Policy as ALLOW
- lambda and codeguru profiling
  - using codeguru profiler for your lambda function to check performance
  - support java and python
- aws lambda limits to know - per region
  - execution
    - max timeout: 15 min
    - environment variables: 4kb
  - deployment
    - zip: 50mb
    - uncompressed: 250mb
    - can use /tmp to load other files at startup
- lambda best practiecs
  - perform heavy-duty work outside of your lambda
  - use env variables
  - minimize deployment package size
  - avoid using recursive code, never have a lambda call itself



#### dynamodb

- nosql db:
  - not support JOIN
  - distributed
  - all the data needed for a query is present in one row
  - no aggregation operations
  - scale horizontally
- overview
  - fully managed
  - HA and scalable
- basics
  - max size of each item: 400kb
  - support: scalar types, document types, set types
- primary key
  - partition key (hash)
  - partition key + sort key (hash + range)
  - must be unique for identifying an item
- read/write capacity modes
  - provisioned mode:
    - read capacity units -- throughput for reads
    - write capacity units -- throughput for writes
    - option for auto-scaling
    - if gets throttled, try `exponential backoff`
  - on-demand mode:
    - read/write auto scaling
    - good for unpredictive workload
  - One Write Capacity Unit (WCU): represents one write per second for an
item up to 1 KB in size. if more than 1kb, then more wcus
  - One Read Capacity Unit (RCU): represents one Strongly Consistent Read per second, or two Eventually Consistent Reads per second, for an item up to 4 KB in size
  - can switch between every 24 hrs
- strongly consistent read vs eventually consistent read
  - eventually consistent read (default)
  - strongly consistent read: set `consistentread` parameter; will consume double rcu
- partitions internal
  - data stored in partitions
  - Partition Keys go through a hashing algorithm to know to which partition they go to
  - to compute the number of partitions:
    - by capacity
    - by size
    - then check the max one (by capacity, by size)
    - wcu and rcu are spread evenly across partitions
- throttling
  - reasons:
    - hot keys
    - hot partitions
    - very large items (rcu, wcu depend on the size of item)
  - solutions:
    - exponential backoff
    - distribute partition keys more
    - if RCU issue, can use dynamodb accelerator DAX
- writing data
  - putItem: same primary key will be overwritten
  - updateItem
  - conditional write: help with concurrent access, no performance agent
- reading data
  - getItem:
    - eventually consistent read (default)
    - hash / hash + range
    - projectionExpression
  - query:
    - keyConditionExpression
    - filterExpression: after query operation, only used with non-key attributes
    - return up to 1mb data
    - able to paginate
  - scan:
    - consumer a lot rcu
    - can perform `parallel scan`
    - can use `projectionExpression` and `filterExpression`, but no change to rcu
- deleting data
  - delete item
  - delete table
- batch operations
  - allow to reduce the number of api calls
  - operations are done in parallel
  - for partial failure, we need to retry them
  - batchwriteitem:
    - up to 25 items
    - unprocessedkeys
  - batchgetitem:
    - up to 100 items
    - unprocessedkeys
- dynamodb -- partiql
  - sql compatible query language for dynamodb
  - using sdk, cli, aws console, dynamodb api,
- conditional writes
  - attribute_exists
  - attribute_not_exists: make sure item is not overwritten, used for partition key or partition key and sort key
  - attribute_type
  - contains (string): check substring
  - begins_with (string): check prefix
  - size (string length)
  - in, and, between,
- local secondary index
  - alternative sort key (same partition key)
  - up to 5 local secondary indexes
  - must be defined at the table creation time
  - can choose which attribute to project -- keys_only, all, include 
- global secondary index
  - alternative primary key (partition key or partition key + sort key)
  - query on non-key attributes
  - can choose which attribute to project -- keys_only, all, include
  - can be added/updated after table creation
  - should provision rcu, wcu 
- indexes and throttling
  - GSI:
    - if write is throttled on gsi, then main table will be throttled
    - even if wcu on main table is fine!!!!
    - choose gsi partition key carefully
    - assign wcu carefully
  - LSI:
    - use wcu and rcu of the main table
    - no special throttling considerations
- partiql
  - sql-compatible
  - support batch operations
- optimistic locking
  - ddb has a feature called 'conditional writes'
  - check an attribute, like `version number`
  - optimistic concurrency control (OCC)
- dynamodb  accelerator (DAX)
  - fully managed service
  - seamless in-memory cache for ddb
  - no application logic modification
  - help offload read workload, solve the hot key issue
  - up to 10 nodes in the cluster
  - multi-az
  - secure
  - vs elasticache
    - store individual objects cache
    - elasticache: store aggregation result
- dynamodb streams
  - ordered stream of item-level modifications in a table
  - downstream:
    - kds
    - lambda
    - kcl
  - retention: 24 hrs
  - use cases:
    - cross-region replication
    - into opensearch
    - analytics
    - react to changes in real-time
  - able to choose what information will be written into stream
    - keys
    - new
    - old
    - new and old
  - made of shards, just like kinesis data streams
  - with lambda
    - using event source mapping
    - invoked  synchronously
- time to live
  - auto delete item after an expiry timestamp
  - no extra cost
  - must be a "number" data type of "unix epoch timestamp" value
  - deletion within 48 hrs
  - the deletion operation for each expired item enters the dynamodb stream
- dynamodb cli
- dynamodb transactions
  - ACID
  - read modes
  - write modes
  - consume 2x wcu and rcu
    - dynamodb perform 2 operations for every item (prepare and commit)
  - two operations
    - transactgetitems
    - transactwriteitems
- ddb as session state cache
  - common use
  - elasticache: not serverless
  - efs: must attached to ec2 as network drive
  - ebs and instance store: only for local cache
  - s3: not for small objects, high latency
- write sharding
  - add suffixes to make partition keys more distributed
- write types
  - concurrent writes
  - atomic writes
  - conditional writes
  - batch writes
- large objects pattern
  - with s3, only store metadata
- indexing s3 object metadata
  - could use s3 event notification to call a lambda to store object metadata
- dynamodb operations
  - table cleanup
    - scan + delete: expensive
    - drop + recreate: fast and cheap
  - copying a dynamodb table
    - **aws data pipeline**
    - backup and restore: takes time
    - scan + putitem or batchwriteitem : write custom code
- security and other features
  - security:
  - backup and restore:
    - PITR
    - no performance impact
  - global table
  - dynamodb local: good for dev and test
  - aws database migration service: only for target, not for source
- fine-grained access control
  - for federated or cognito identity pool: get sts token
  - can assign iam role
  - leadingKeys -- limit row-level access on primary key
  - attributes -- limit specific attributes that users can see



#### api gateway

- integration:
  - lambda
  - http
  - aws service
- endpoint types
  - edge-optimized (default)
    - cloudfront edge locations
    - api gateway lives in one region
  - regional:
    - can manually combine with cloufront distro
  - private:
    - only within your vpc
- security:
  - iam roles
  - cognito
  - custom authorizer
- deployment stages
  - making changes for the api gateway does not mean they are effective
  - need to make a deployment for them to be in effect
  - each stage has its own configuration parameters
  - stage can roll back
- stage variables
  - like environment variables for api gateway
  - used in
    - lambda arn
    - http endpoint
    - parameter mapping templates
  - use cases:
    - configure http endpoints your stages talk to (dev,test,prod,...)
    - pass configuration parameters to lambda through mapping templates
  - stage variables are passed to the `context` object in lambda
  - format: ${stageVariables.variableName}
- api gateway stage variables & lambda aliases
  - create stage variables to indicate the corresponding lambda aliases
  - then api gateway will automatically invoke the right lambda function
- api gateway -- canary deployment
  - usually prod stage
  - split a certain percentage of traffic to the canary channel
  - this is the blue/green deployment with lambda and api gateway
- integration types
  - MOCK: returns a response without sending requests to the backend
  - HTTP/AWS(lambda or other services): setup data mapping using mapping template
  - AWS_Proxy (lambda proxy): incoming requests will be forwarded to the lambda
    - no mapping template, headers, query string parameters,... are passed as arguments
  - http_proxy:
    - no mapping template
    - http request passed to the backend
    - possible to add http headers
- mapping templates (aws & http integration)
  - used to modify request/response
  - add headers
  - modify body content
  - rename/modify query string parameters
  - uses Velecity template language(vtl) or javascript
  - filter out results (remove unnecessary data)
  - content-type: application/json or application/xml
  - can convert rest json to  SOAP xml, build the soap message and call soap service with the message, then transform soap response back to rest json response
- open api spec
  - can export current api as openapi spec
  - can be written in yml or json
  - using open api to generate sdk for our applications
- rest api -- request validation
  - can configure api gateway to perform basic validation without calling the backend
- caching api responses
  - defined per stage
  - possible to override cache settings per method
  - encryption option
  - expensive, use it only in prod
- cache invalidation
  - able to flush entire cache
  - invalidationCache policy to restrict access
- usage plans & api keys
  - make api profitable
  - usage plan:
    - who can access what api stages and methods
    - how mush they can use
    - uses api keys to identify api clients and meter access
    - configure throttling limits and quota limits for individual client
  - api keys:
    - distributed to your customers
    - can use with usage plan to control access
    - throttling limits are applied to api keys
    - quotas limits : overall number of requests
- correct order for api keys
  - to configure a usage plan:
    - create apis, configure methods that need api keys and deploy
    - generate api keys and distribute them to app developers(your customers)
    - create usage plan with the desired throttle and quota limits
    - associate api stage with api keys with usage plan
    - the header: `x-api-key`
- api gateway -- logging & tracing
  - cloudwatch logs:
    - configure log level
    - can override settings on a per api basis
  - x-ray
- api gateway -- cloudwatch metrics
  - cacheHitCount & cacheMissCount
  - count
  - integrationLatency
  - latency
  - 4xx and 5xx
- api gateway throttling
  - account limit
  - can set stage limit & method limits
  - or can define usage plans to throttle per customer
- api gateway -- errors
  - 4xx
  - 5xx
- api gateway -- cors
  - the options pre-flight request must contain the following headers:
    - access-control-allow-methods
    - access-control-allow-headers
    - access-control-allow-origins
- api gateway -- security
  - iam permission to iam roles iam users
  - resource policies
  - cognito user pools
  - lambda authorizer:
    - token-based authorizer
    - a request parameter-based lambda authorizer
- api gateway -- http api vs rest api
  - http apis
  - rest apis
- api gateway -- websocket api
  - bi-directional communication
  - client-server: connecitonID is re-used for different messages
  - server-client: connection url callback
    - post
    - get
    - delete
  - routing
    - no routes -- sent to $default
    - you select a route selection expression to select the field on the json to route from
- api gateway -- architecture
  - create a single interface for all the microservices in your company
  - apply a simple domain name and ssl
  - can apply forwarding and transformation rules at the api gateway level
  


#### cicd

- introduction
  - code commit
    - no longer accept the new customers
    - version control
    - collaboration
    - security
      - authentication
      - authorization
      - encryption
      - cross-account access
  - code pipeline
    - orchesrtate your cicd
    - source -- build -- test -- deploy -- load testing -- ...
    - artifacts:
    - troubleshooting
      - for pipeline/action/stage execution stage changes
      - use eventbridge to create events
      - if pipeline fails, check the console
      - if pipeline cannot perform an action, check iam permission
      - cloudtrail can be used to audit 
  - code build
    - fully managed ci service
    - leverage docker under the hood
    - source: codecommit, github, s3,...
    - build instructions: buildspec.yml
      - env:
        - variables
        - parameter store
        - secrets manager
      - phases
        - install
        - pre-build
        - build
        - post-build
      - artifacts: send to s3 bucket used by next stage
      - cache: files to cache for future build speedup
    - output logs can be stored in s3 or cloudwatch logs
    - use eventbridge to detect failed builds
    - use cloudwatch alarms to notify
  - code deploy
    - deploy applications to
      - ec2, on-prem:
        - perform in-place deployment or blue/green
        - for ec2 and on-prem, codedeploy agent
        - blue/green:
          - new asg
        - deployment speed:
          - all at once
          - half at a time: in-place deployment
          - one a time
          - custom
      - lambda
        - can help do traffic splitting for lambda alias
        - integrated with sam
        - linear
        - canary
        - all at once
      - ecs
        - only blue/green
        - new target group
        - linear
        - canary
        - all at once
      - deployment to ec2
        - appspec.yml + deployment strategy
        - will do in-place deployment
      - deployment to asg
        - in place
        - blue/green: must use ELB
    - redeploy & rollbacks
      - when rollback happens, a new deployment will be created
    - auto rollback
    - gradual deployment control
    - use appspec.yml
  - code star
    - an integrated solution of github, code commit, code build, code deploy, cloudformation, code pipeline, cloudwatch,...
  - code artifact
    - cost-efective artifact management(dependencies) for software development
    - resource policy
  - code guru
    - ml-powered service that automate code review and application performance recommendations
    - codeguru reviewer:
      - support java and python
    - codeguru profiler:
      - help identify your application performance and provides heap summary
      - agent configuration:
        - maxStackDepth
        - MemoryUsageLimitPercent
        - MinimumTimeForReportingInMilliseconds
        - ReportingIntervalInMilliseconds
        - SamplingIntervalInMilliseconds
  - cloud9
    - cloud-based development environment
- continuous integration (ci)
  - push code
  - test/build
  - get feedback
- continuous delivery (cd)
  - ensure deployments happen often and quick



#### serverless application model

- serverless application model
- using yml files to create cloudformaiton templates
- use codedeploy
- recipe
  - transform header: indicates it is sam template
  - quickly sync local changes to aws lambda: sam sync -- watch
  - package & deploy: sam deploy
- sam accelerate(sam sync)
  - synchronize the sam templates to aws
  - synchronize code changes to aws without updating infra (bypass cloudformation)
- sam -- cli debugging
  - sam cli
  - aws toolkit
  - provide lambda-like execution environment locally
- sam policy templates
  - list of templates to apply permissions to your lambda functions
- sam and codedeploy
  - natively uses code deploy to update lambda
  - traffic shifting feature
  - pre and post traffic hooks
  - easy and auto rollback using cloudwatch alarms
  - **note**:
    - AutoPublishAlias: create new versions and point the lambda alias to the latest version
    - DeploymentPreference
    - Alarms
    - Hooks
- sam -- local capabilities
  - start lambda locally
  - invoke lambda locally
  - start an api gateway endpoint
  - generate aws events for lambda functions
- sam -- multi environments
  - in samconfig.toml, deploy resources in multi environments



#### cdk

- overview:
  - using high level components called constructs
  - compared to sam: can provision all aws services
- to use sam to test cdk apps, should run cdk synth
- cdk constructs
  - components including everything cdk needs to create a final cloudformation stack
  - aws construct library:
    - contains 3 different levels of constructs
  - construct hub
    - from aws, 3rd parties and open-source
- constructs -- layer1
  - pure cloudformation resources
  - names start with Cfn
- constructs -- layer2
  - similar functionality but with convinent defaults and boilerplate
  - provide methods to make it simpler to work with
- construct -- layer3
  - called patterns, which represents multi resoures
  - less customization
  - common tasks
- cdk -- bootstrapping
  - The process of provisioning resources for CDK before you can deploy CDK apps into an AWS environment
  - cdktoolkit stack: s3 bucket, iam roles
- cdk -- testing
  - To test CDK apps, use `CDK Assertions Module` combined with popular test frameworks such as Jest (JavaScript) or Pytest (Python)
  - two types of tests:
    - fine-grained assertions(common)
    - snapshot tests
  - import a template
    - Template.fromStack(MyStack) : stack built in CDK
    - Template.fromString(mystring) : stack build outside CDK

  

#### cognito

- overview:
  - cognito user pool
    - authentication
    - serverless data for users for web, mobile apps
    - mfa
    - email & phone
    - password reset
    - federated identities
    - block users if their credentials are compromissed
    - integrations:
      - api gateway
      - ALB
    - triggers
    - hosted authentication UI can be added to apps
    - hosted ui custom domain
      - for custom domains, must create acm certificate in us-east-1
      - custom domain must be defined in the `app integration` section
    - adaptive authentication
      - block sign-ins or require mfa if suspicious
      - examie each sign-in, generate a risk score
      - integrated with cloudwatch logs
    - decoding ID token
      - the token signature must be verified to ensure jwt can be trusted
      - libraries can help you verify the token
      - the payload contains the user info
    - ALB -- authenticate users
      - must use an https listener to set authenticate-oidc or cognito
      - from 3rd identity providers
      - or cognito user pools
      - auth through cognito user pools
      - oidc auth
      - idp (oidc compliant)
  - cognito identity pool (federated identity)
    - get identities for users so they can obtain temoprary aws credentials
    - users can access aws services directly or through api gateway
    - your identity pool can include:
      - public providers
      - cognito user pool
      - oidc or saml
      - allow guess access
    - iam roles
      - default for authenticated and guest users
      - define rules to choose role for each user based on their ID
      - can partition users access using `policy variables`
      - roles must have a trust policy of cognito identity pool
- cognito user pool vs cognito identity pool
  - cognito user pool: authentication
  - cognito identity pool: authorization

    

#### other serverless

- step functions
  - model your workflow as state machines
  - written in json
  - visualization
  - start workflow with sdk, api gateway, eventbridge
- step function -- task states
  - do some work in your state machine
  - can invoke one aws service
  - run one activity
- step function -- states
  - choice state -- branch/condition
  - fail or success state -- stop execution with failure/success
  - pass state -- pass input to output or inject some fixed data without performing work
  - wait state -- provide a delay or until a specific time
  - map state -- dynamic iterate steps
  - parallel state -- begin parallel branches of execution
- step function -- error handling
  - use retry
  - use catch
  - pre-defined error codes:
    - states.all
    - states.timeout
    - states.taskfailed
    - states.permissions
  - state may report its own errors
- step function -- retry (task or parallel state)
  - evaluate from top to bottom
  - errorEquals
  - intervalSeconds
  - backoffRate
  - maxAttempts
    - catch when max attempts are reached
- step function -- catch (task or parallel state)
  - evaluate from top to bottom
  - errorEquals
  - next
  - resultPath
- step function -- resultPath
  - include error in the input
- step function -- wait for task token
  - allow to pause state machine until a task token is returned
  - task can wait for other aws services, human approval, 3rd party integration, call legacy system....
  - append `.waitForTaskToken` to the `resource` field to tell step functions to wait for the task token
  - sendTaskSuccess or sendTaskFailure 
- step function -- activity tasks
  - task work polled and performed by activity worker
  - the worker can run on ec2, lambda, mobile device,...
  - once finish the work, send a response back using sendTaskSuccess or sendTaskFailure
  - to keep the task active:
    - how long a task can wait by setting timeout
    - periodically send a heartbeat
    - can wait up to 1 year
- step function -- standard vs express
- aws appsync
  - aws managed graphql service
  - can combine data from multiple sources
  - work with websocket or mqtt
  - all starts with uploading one graphql schema2
- appsync security
  - api key
  - iam
  - oicd
  - cognito user pool
- aws amplify
  - amplify studio
  - amplify cli
  - amplify libraries
  - amplify hosting
    - built-in cicd
    - pull request preview
    - using cloudfront
    - e2e testing in test phase of cicd, integrated with cypress




#### advanced identity
#### security and encryption
#### other services

- aws ses
  - fully managed
  - allow inbound/outbound emails
  - reputation dashboard, anti-spam feedback, performance insights
  - Supports DomainKeys Identified Mail (DKIM) and Sender Policy Framework (SPF)
  - Flexible IP deployment: shared, dedicated, and customer-owned IPs
  - Send emails using your application using AWS Console, APIs, or SMTP
  - Use cases: transactional, marketing and bulk email communications
- aws opensearch service
  - successor to aws elasticsearch
  - with opensearch, we can search any field, or partially matches
  - complement to other databases like dynamodb
  - managed cluster or serverless cluster
  - does not natively support SQL (via a plugin)
  - come with dahsboard
  - integrated with kdf, iot, cloudwatch logs
  - kds(real time with a lambda) or kdf(near real time with a lambda doing data transformation)
  - cloudwatch logs: subscription filter + lambda/kdf
- aws athena
  - serverless query service --> s3
  - using standard sql
  - used with quicksight for data visualization
  - performance improvement:
    - columnar data(less scan)
    - compress data for smaller retrievals
    - use larger file(> 128mb) to minimize overhead
    - partition datasets
  - federated query
    - use `data source connector` running on aws lambda
- aws managed streaming for apache kafka
  - alternative to aws kinesis
  - fully managed kafka on aws
    - allow to create, update, delete clusters
  - msk serverless
    - without mananing the capacity
  - aws kds vs aws msk
    - kds:
      - 1mb message size
      - data stream with shards
      - shard splitting & merging
      - encryption in-transit and at rest
    - msk:
      - 1mb default, up to 10mb message size
      - kafka topics with partitions
      - can only add partition to a topic
      - encryption in-transit and at rest
    - consumers:
      - kda
      - aws glue for ETL
      - lambda
      - ec2, ecs,eks
- aws acm
  - ssl/tls certificates
  - auto tls renewal
  - integrated with
    - EB
    - cloudfront distro
    - api gateway
    - alb     
- aws private certificate authority (CA)
  - managed service to create private CA (root, subordinaries)
  - issue x.509 certificates
  - certificates only trusted within your organization(not public)
  - work for the services that are integrated with acm
  - use cases:
    - authenticate users, computers, api endpoints, iot
    - enterprises build public key infra
    - encrypt tls communication, cryptographically signing code
- aws macie
  - detect PII or users sensitive data in s3
- aws appConfig
  - configure, validate, and deploy dynamic configurations
    - no need to restart the application
  - used with apps on ec2, lambda, ecs, eks,...
  - gradually deploy changes and rollback if need
- cloudwatch evidently
  - Safely validate new features by serving them to a specified % of your users
  - Launches (= feature flags): enable and disable features for a subset of users
  - Experiments (= A/B testing): compare multiple versions of the same feature
  - Overrides: pre-define a variation for a specific user
  - Store evaluation events in CloudWatch Logs or S3


### lecture hands on


### practice tests

- practice exam:
  - 1st try: 61% (40/65)

### review lectures and tests







